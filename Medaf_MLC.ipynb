{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a2bca86-a8bf-4528-b917-1d9a07411aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override: MultiLabelMEDAF (from core.multilabel_net.py)\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from core.net import build_backbone, conv1x1, Classifier\n",
    "\n",
    "class MultiLabelMEDAF(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Label version of MEDAF (Multi-Expert Diverse Attention Fusion)\n",
    "\n",
    "    Key changes from original MEDAF:\n",
    "    1. Support for multi-hot label targets\n",
    "    2. BCEWithLogitsLoss instead of CrossEntropyLoss\n",
    "    3. Multi-label attention diversity computation\n",
    "    4. Per-sample CAM extraction for multiple positive classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args=None):\n",
    "        super(MultiLabelMEDAF, self).__init__()\n",
    "        backbone, feature_dim, self.cam_size = build_backbone(\n",
    "            img_size=args[\"img_size\"],\n",
    "            backbone_name=args[\"backbone\"],\n",
    "            projection_dim=-1,\n",
    "            inchan=3,\n",
    "        )\n",
    "        self.img_size = args[\"img_size\"]\n",
    "        self.gate_temp = args[\"gate_temp\"]\n",
    "        self.num_classes = args[\"num_classes\"]\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Shared layers (L1-L3)\n",
    "        self.shared_l3 = nn.Sequential(*list(backbone.children())[:-6])\n",
    "\n",
    "        # Expert branch 1\n",
    "        self.branch1_l4 = nn.Sequential(*list(backbone.children())[-6:-3])\n",
    "        self.branch1_l5 = nn.Sequential(*list(backbone.children())[-3])\n",
    "        self.branch1_cls = conv1x1(feature_dim, self.num_classes)\n",
    "\n",
    "        # Expert branch 2 (deep copy)\n",
    "        self.branch2_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.branch2_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        self.branch2_cls = conv1x1(feature_dim, self.num_classes)\n",
    "\n",
    "        # Expert branch 3 (deep copy)\n",
    "        self.branch3_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.branch3_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        self.branch3_cls = conv1x1(feature_dim, self.num_classes)\n",
    "\n",
    "        # Gating network\n",
    "        self.gate_l3 = copy.deepcopy(self.shared_l3)\n",
    "        self.gate_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.gate_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        self.gate_cls = nn.Sequential(\n",
    "            Classifier(feature_dim, int(feature_dim / 4), bias=True),\n",
    "            Classifier(int(feature_dim / 4), 3, bias=True),  # 3 experts\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y=None, return_ft=False):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-label MEDAF\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor [B, C, H, W]\n",
    "            y: Multi-hot labels [B, num_classes] or None\n",
    "            return_ft: Whether to return features\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing logits, gate predictions, and CAMs/features\n",
    "        \"\"\"\n",
    "        b = x.size(0)\n",
    "        ft_till_l3 = self.shared_l3(x)\n",
    "\n",
    "        # Expert branch 1\n",
    "        branch1_l4 = self.branch1_l4(ft_till_l3.clone())\n",
    "        branch1_l5 = self.branch1_l5(branch1_l4)\n",
    "        b1_ft_cams = self.branch1_cls(branch1_l5)  # [B, num_classes, H, W]\n",
    "        b1_logits = self.avg_pool(b1_ft_cams).view(b, -1)\n",
    "\n",
    "        # Expert branch 2\n",
    "        branch2_l4 = self.branch2_l4(ft_till_l3.clone())\n",
    "        branch2_l5 = self.branch2_l5(branch2_l4)\n",
    "        b2_ft_cams = self.branch2_cls(branch2_l5)  # [B, num_classes, H, W]\n",
    "        b2_logits = self.avg_pool(b2_ft_cams).view(b, -1)\n",
    "\n",
    "        # Expert branch 3\n",
    "        branch3_l4 = self.branch3_l4(ft_till_l3.clone())\n",
    "        branch3_l5 = self.branch3_l5(branch3_l4)\n",
    "        b3_ft_cams = self.branch3_cls(branch3_l5)  # [B, num_classes, H, W]\n",
    "        b3_logits = self.avg_pool(b3_ft_cams).view(b, -1)\n",
    "\n",
    "        # Store CAMs for diversity loss computation\n",
    "        cams_list = [b1_ft_cams, b2_ft_cams, b3_ft_cams]\n",
    "\n",
    "        # Multi-label CAM extraction for positive classes\n",
    "        if y is not None:\n",
    "            multi_label_cams = self._extract_multilabel_cams(cams_list, y)\n",
    "        else:\n",
    "            multi_label_cams = None\n",
    "\n",
    "        if return_ft:\n",
    "            # Aggregate features from all experts\n",
    "            fts = (\n",
    "                b1_ft_cams.detach().clone()\n",
    "                + b2_ft_cams.detach().clone()\n",
    "                + b3_ft_cams.detach().clone()\n",
    "            )\n",
    "\n",
    "        # Gating network\n",
    "        gate_l5 = self.gate_l5(self.gate_l4(self.gate_l3(x)))\n",
    "        gate_pool = self.avg_pool(gate_l5).view(b, -1)\n",
    "        gate_pred = F.softmax(self.gate_cls(gate_pool) / self.gate_temp, dim=1)\n",
    "\n",
    "        # Adaptive fusion using gating weights\n",
    "        gate_logits = torch.stack(\n",
    "            [b1_logits.detach(), b2_logits.detach(), b3_logits.detach()], dim=-1\n",
    "        )\n",
    "        gate_logits = gate_logits * gate_pred.view(\n",
    "            gate_pred.size(0), 1, gate_pred.size(1)\n",
    "        )\n",
    "        gate_logits = gate_logits.sum(-1)\n",
    "\n",
    "        logits_list = [b1_logits, b2_logits, b3_logits, gate_logits]\n",
    "\n",
    "        if return_ft and y is None:\n",
    "            outputs = {\n",
    "                \"logits\": logits_list,\n",
    "                \"gate_pred\": gate_pred,\n",
    "                \"fts\": fts,\n",
    "                \"cams_list\": cams_list,\n",
    "            }\n",
    "        else:\n",
    "            outputs = {\n",
    "                \"logits\": logits_list,\n",
    "                \"gate_pred\": gate_pred,\n",
    "                \"multi_label_cams\": multi_label_cams,\n",
    "                \"cams_list\": cams_list,\n",
    "            }\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _extract_multilabel_cams(self, cams_list, targets):\n",
    "        \"\"\"\n",
    "        Extract CAMs for all positive classes in multi-label setting\n",
    "        \"\"\"\n",
    "        batch_size = targets.size(0)\n",
    "        extracted_cams = []\n",
    "\n",
    "        for expert_idx, expert_cams in enumerate(cams_list):\n",
    "            expert_extracted = []\n",
    "\n",
    "            for batch_idx in range(batch_size):\n",
    "                # Find positive class indices for this sample\n",
    "                positive_classes = torch.where(targets[batch_idx] == 1)[0]\n",
    "\n",
    "                if len(positive_classes) > 0:\n",
    "                    # Extract CAMs for positive classes\n",
    "                    sample_cams = expert_cams[\n",
    "                        batch_idx, positive_classes\n",
    "                    ]  # [num_positive, H, W]\n",
    "                    expert_extracted.append(sample_cams)\n",
    "                else:\n",
    "                    # If no positive classes, create zero tensor\n",
    "                    H, W = expert_cams.shape[-2:]\n",
    "                    expert_extracted.append(\n",
    "                        torch.zeros(1, H, W, device=expert_cams.device)\n",
    "                    )\n",
    "\n",
    "            extracted_cams.append(expert_extracted)\n",
    "\n",
    "        return extracted_cams\n",
    "\n",
    "    def get_params(self, prefix=\"extractor\"):\n",
    "        \"\"\"Get model parameters for different learning rates\"\"\"\n",
    "        extractor_params = (\n",
    "            list(self.shared_l3.parameters())\n",
    "            + list(self.branch1_l4.parameters())\n",
    "            + list(self.branch1_l5.parameters())\n",
    "            + list(self.branch2_l4.parameters())\n",
    "            + list(self.branch2_l5.parameters())\n",
    "            + list(self.branch3_l4.parameters())\n",
    "            + list(self.branch3_l5.parameters())\n",
    "            + list(self.gate_l3.parameters())\n",
    "            + list(self.gate_l4.parameters())\n",
    "            + list(self.gate_l5.parameters())\n",
    "        )\n",
    "        extractor_params_ids = list(map(id, extractor_params))\n",
    "        classifier_params = filter(\n",
    "            lambda p: id(p) not in extractor_params_ids, self.parameters()\n",
    "        )\n",
    "\n",
    "        if prefix in [\"extractor\", \"extract\"]:\n",
    "            return extractor_params\n",
    "        elif prefix in [\"classifier\"]:\n",
    "            return classifier_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5aa59c0-0a12-480d-8e89-c7c2cd1b26d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c1f6fc-b544-4d5c-a149-aa1b20ee121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_LABELS = [\n",
    "    \"Atelectasis\",\n",
    "    \"Cardiomegaly\",\n",
    "    \"Effusion\",\n",
    "    \"Infiltration\",\n",
    "    \"Mass\",\n",
    "    \"Nodule\",\n",
    "    \"Pneumonia\",\n",
    "    \"Pneumothorax\",\n",
    "]\n",
    "\n",
    "DEFAULT_IMAGE_ROOT = Path(\"datasets/data/chestxray/NIH/images-224\")\n",
    "DEFAULT_KNOWN_CSV = Path(\"datasets/data/NIH/chestxray/chestxray_train_known.csv\")\n",
    "DEFAULT_CHECKPOINT_DIR = Path(\"checkpoints/medaf_phase1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef026e-5abb-4332-a13c-8c8a69d0db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayKnownDataset(data.Dataset):\n",
    "    \"\"\"Dataset that reads ChestX-ray14 samples from the known-label CSV split.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path,\n",
    "        image_root,\n",
    "        img_size=224,\n",
    "        max_samples=64,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.image_root = Path(image_root)\n",
    "        self.img_size = img_size\n",
    "\n",
    "        if not self.csv_path.exists():\n",
    "            raise FileNotFoundError(f\"ChestX-ray CSV not found: {self.csv_path}\")\n",
    "        if not self.image_root.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"ChestX-ray image directory not found: {self.image_root}\"\n",
    "            )\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((img_size, img_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    # transforms.Normalize(\n",
    "                    #     mean=[0.485, 0.456, 0.406],\n",
    "                    #     std=[0.229, 0.224, 0.225],\n",
    "                    # ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(KNOWN_LABELS)}\n",
    "        self.num_classes = len(self.label_to_idx)\n",
    "\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        if \"known_labels\" not in df.columns:\n",
    "            raise ValueError(\n",
    "                \"Expected 'known_labels' column in CSV. Run create_chestxray_splits.py first.\"\n",
    "            )\n",
    "\n",
    "        if max_samples is not None and max_samples < len(df):\n",
    "            df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "        self.records = df.to_dict(\"records\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_label_list(raw_value):\n",
    "        if isinstance(raw_value, list):\n",
    "            return raw_value\n",
    "        if pd.isna(raw_value):\n",
    "            return []\n",
    "        if isinstance(raw_value, str):\n",
    "            raw_value = raw_value.strip()\n",
    "            if not raw_value:\n",
    "                return []\n",
    "            try:\n",
    "                parsed = ast.literal_eval(raw_value)\n",
    "                if isinstance(parsed, (list, tuple)):\n",
    "                    return list(parsed)\n",
    "                if isinstance(parsed, str):\n",
    "                    return [parsed]\n",
    "            except (ValueError, SyntaxError):\n",
    "                pass\n",
    "            return [item.strip() for item in raw_value.split(\"|\") if item.strip()]\n",
    "        return []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.records[idx]\n",
    "        image_path = self.image_root / record[\"Image Index\"]\n",
    "        if not image_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing image: {image_path}\")\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        labels = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        for label in self._parse_label_list(record.get(\"known_labels\", [])):\n",
    "            if label in self.label_to_idx:\n",
    "                labels[self.label_to_idx[label]] = 1.0\n",
    "\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d01ee3-69db-4213-bcaf-a2f3fbee62c3",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "# Demo configuration\n",
    "config = {\n",
    "    \"data_source\": \"chestxray\",\n",
    "    \"known_csv\": str(DEFAULT_KNOWN_CSV),\n",
    "    \"image_root\": str(DEFAULT_IMAGE_ROOT),\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"val_ratio\": 0.1,\n",
    "    \"num_workers\": 2,\n",
    "    # \"max_samples\": None,  # Set to an int for quicker experiments\n",
    "    \"max_samples\": 1000,\n",
    "    \"phase1_checkpoint\": \"medaf_phase1_chestxray.pt\",\n",
    "    \"checkpoint_dir\": str(DEFAULT_CHECKPOINT_DIR),\n",
    "    \"run_phase2\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e9f237-b4aa-404f-8f5a-a3d0e71b026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = config.get(\"data_source\", \"chestxray\").lower()\n",
    "\n",
    "if data_source == \"chestxray\":\n",
    "    csv_path = Path(self.config.get(\"known_csv\", DEFAULT_KNOWN_CSV))\n",
    "    image_root = Path(self.config.get(\"image_root\", DEFAULT_IMAGE_ROOT))\n",
    "    max_samples = self.config.get(\"max_samples\")\n",
    "    if isinstance(max_samples, str):\n",
    "        max_samples = int(max_samples)\n",
    "    print(f\"Loading ChestX-ray14 known-label split from {csv_path}\")\n",
    "    dataset = ChestXrayKnownDataset(\n",
    "        csv_path=csv_path,\n",
    "        image_root=image_root,\n",
    "        img_size=self.config.get(\"img_size\", 224),\n",
    "        max_samples=max_samples,\n",
    "    )\n",
    "    self.dataset_name = \"ChestX-ray14 (known labels)\"\n",
    "    self.class_names = dataset.class_names\n",
    "    self.config[\"num_classes\"] = dataset.num_classes\n",
    "    self.config[\"img_size\"] = dataset.img_size\n",
    "\n",
    "val_ratio = float(self.config.get(\"val_ratio\", 0.1))\n",
    "val_size = max(1, int(len(dataset) * val_ratio))\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_dataset, val_dataset = data.random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")\n",
    "\n",
    "batch_size = self.config.get(\"batch_size\", 16)\n",
    "num_workers = self.config.get(\"num_workers\", 4)\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "self.train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "self.val_loader = data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "self.test_loader = self.val_loader\n",
    "\n",
    "print(\n",
    "    f\"Dataset prepared: {train_size} train / {val_size} val samples ({self.dataset_name})\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research_medaf_aidan)",
   "language": "python",
   "name": "research_medaf_aidan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
