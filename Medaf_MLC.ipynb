{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a2bca86-a8bf-4528-b917-1d9a07411aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from core.net import build_backbone, conv1x1, Classifier\n",
    "\n",
    "\n",
    "class MultiLabelMEDAF(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Label version of MEDAF (Multi-Expert Diverse Attention Fusion)\n",
    "\n",
    "    Key changes from original MEDAF:\n",
    "    1. Support for multi-hot label targets\n",
    "    2. BCEWithLogitsLoss instead of CrossEntropyLoss\n",
    "    3. Multi-label attention diversity computation\n",
    "    4. Per-sample CAM extraction for multiple positive classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args=None):\n",
    "        super(MultiLabelMEDAF, self).__init__()\n",
    "        backbone, feature_dim, self.cam_size = build_backbone(\n",
    "            img_size=args[\"img_size\"],\n",
    "            backbone_name=args[\"backbone\"],\n",
    "            projection_dim=-1,\n",
    "            inchan=3,\n",
    "        )\n",
    "        self.img_size = args[\"img_size\"]\n",
    "        self.gate_temp = args[\"gate_temp\"]\n",
    "        self.num_classes = args[\n",
    "            \"num_classes\"\n",
    "        ]  # Changed from num_known to num_classes for clarity\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Shared layers (L1-L3)\n",
    "        self.shared_l3 = nn.Sequential(*list(backbone.children())[:-6])\n",
    "\n",
    "        # Expert branch 1\n",
    "        self.branch1_l4 = nn.Sequential(*list(backbone.children())[-6:-3])\n",
    "        self.branch1_l5 = nn.Sequential(*list(backbone.children())[-3])\n",
    "        self.branch1_cls = conv1x1(feature_dim, self.num_classes)\n",
    "\n",
    "        # Expert branch 2 (deep copy)\n",
    "        self.branch2_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.branch2_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        self.branch2_cls = conv1x1(feature_dim, self.num_classes)\n",
    "\n",
    "        # Expert branch 3 (deep copy)\n",
    "        self.branch3_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.branch3_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        self.branch3_cls = conv1x1(feature_dim, self.num_classes)\n",
    "\n",
    "        # Gating network\n",
    "        self.gate_l3 = copy.deepcopy(self.shared_l3)\n",
    "        self.gate_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.gate_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        self.gate_cls = nn.Sequential(\n",
    "            Classifier(feature_dim, int(feature_dim / 4), bias=True),\n",
    "            Classifier(int(feature_dim / 4), 3, bias=True),  # 3 experts\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y=None, return_ft=False):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-label MEDAF\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor [B, C, H, W]\n",
    "            y: Multi-hot labels [B, num_classes] or None\n",
    "            return_ft: Whether to return features\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing logits, gate predictions, and CAMs/features\n",
    "        \"\"\"\n",
    "        b = x.size(0)\n",
    "        ft_till_l3 = self.shared_l3(x)\n",
    "\n",
    "        # Expert branch 1\n",
    "        branch1_l4 = self.branch1_l4(ft_till_l3.clone())\n",
    "        branch1_l5 = self.branch1_l5(branch1_l4)\n",
    "        b1_ft_cams = self.branch1_cls(branch1_l5)  # [B, num_classes, H, W]\n",
    "        b1_logits = self.avg_pool(b1_ft_cams).view(b, -1)\n",
    "\n",
    "        # Expert branch 2\n",
    "        branch2_l4 = self.branch2_l4(ft_till_l3.clone())\n",
    "        branch2_l5 = self.branch2_l5(branch2_l4)\n",
    "        b2_ft_cams = self.branch2_cls(branch2_l5)  # [B, num_classes, H, W]\n",
    "        b2_logits = self.avg_pool(b2_ft_cams).view(b, -1)\n",
    "\n",
    "        # Expert branch 3\n",
    "        branch3_l4 = self.branch3_l4(ft_till_l3.clone())\n",
    "        branch3_l5 = self.branch3_l5(branch3_l4)\n",
    "        b3_ft_cams = self.branch3_cls(branch3_l5)  # [B, num_classes, H, W]\n",
    "        b3_logits = self.avg_pool(b3_ft_cams).view(b, -1)\n",
    "\n",
    "        # Store CAMs for diversity loss computation\n",
    "        cams_list = [b1_ft_cams, b2_ft_cams, b3_ft_cams]\n",
    "\n",
    "        # Multi-label CAM extraction for positive classes\n",
    "        if y is not None:\n",
    "            # Extract CAMs for all positive classes across all experts\n",
    "            # This will be used for attention diversity computation\n",
    "            multi_label_cams = self._extract_multilabel_cams(cams_list, y)\n",
    "        else:\n",
    "            multi_label_cams = None\n",
    "\n",
    "        if return_ft:\n",
    "            # Aggregate features from all experts\n",
    "            fts = (\n",
    "                b1_ft_cams.detach().clone()\n",
    "                + b2_ft_cams.detach().clone()\n",
    "                + b3_ft_cams.detach().clone()\n",
    "            )\n",
    "\n",
    "        # Gating network\n",
    "        gate_l5 = self.gate_l5(self.gate_l4(self.gate_l3(x)))\n",
    "        gate_pool = self.avg_pool(gate_l5).view(b, -1)\n",
    "        gate_pred = F.softmax(self.gate_cls(gate_pool) / self.gate_temp, dim=1)\n",
    "\n",
    "        # Adaptive fusion using gating weights\n",
    "        gate_logits = torch.stack(\n",
    "            [b1_logits.detach(), b2_logits.detach(), b3_logits.detach()], dim=-1\n",
    "        )\n",
    "        gate_logits = gate_logits * gate_pred.view(\n",
    "            gate_pred.size(0), 1, gate_pred.size(1)\n",
    "        )\n",
    "        gate_logits = gate_logits.sum(-1)\n",
    "\n",
    "        logits_list = [b1_logits, b2_logits, b3_logits, gate_logits]\n",
    "\n",
    "        if return_ft and y is None:\n",
    "            outputs = {\n",
    "                \"logits\": logits_list,\n",
    "                \"gate_pred\": gate_pred,\n",
    "                \"fts\": fts,\n",
    "                \"cams_list\": cams_list,\n",
    "            }\n",
    "        else:\n",
    "            outputs = {\n",
    "                \"logits\": logits_list,\n",
    "                \"gate_pred\": gate_pred,\n",
    "                \"multi_label_cams\": multi_label_cams,\n",
    "                \"cams_list\": cams_list,\n",
    "            }\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _extract_multilabel_cams(self, cams_list, targets):\n",
    "        \"\"\"\n",
    "        Extract CAMs for all positive classes in multi-label setting\n",
    "\n",
    "        Args:\n",
    "            cams_list: List of CAMs from 3 experts [B, num_classes, H, W]\n",
    "            targets: Multi-hot labels [B, num_classes]\n",
    "\n",
    "        Returns:\n",
    "            extracted_cams: List of CAMs for positive classes per expert\n",
    "        \"\"\"\n",
    "        batch_size = targets.size(0)\n",
    "        extracted_cams = []\n",
    "\n",
    "        for expert_idx, expert_cams in enumerate(cams_list):\n",
    "            expert_extracted = []\n",
    "\n",
    "            for batch_idx in range(batch_size):\n",
    "                # Find positive class indices for this sample\n",
    "                positive_classes = torch.where(targets[batch_idx] == 1)[0]\n",
    "\n",
    "                if len(positive_classes) > 0:\n",
    "                    # Extract CAMs for positive classes\n",
    "                    sample_cams = expert_cams[\n",
    "                        batch_idx, positive_classes\n",
    "                    ]  # [num_positive, H, W]\n",
    "                    expert_extracted.append(sample_cams)\n",
    "                else:\n",
    "                    # If no positive classes, create zero tensor\n",
    "                    H, W = expert_cams.shape[-2:]\n",
    "                    expert_extracted.append(\n",
    "                        torch.zeros(1, H, W, device=expert_cams.device)\n",
    "                    )\n",
    "\n",
    "            extracted_cams.append(expert_extracted)\n",
    "\n",
    "        return extracted_cams\n",
    "\n",
    "    def get_params(self, prefix=\"extractor\"):\n",
    "        \"\"\"Get model parameters for different learning rates\"\"\"\n",
    "        extractor_params = (\n",
    "            list(self.shared_l3.parameters())\n",
    "            + list(self.branch1_l4.parameters())\n",
    "            + list(self.branch1_l5.parameters())\n",
    "            + list(self.branch2_l4.parameters())\n",
    "            + list(self.branch2_l5.parameters())\n",
    "            + list(self.branch3_l4.parameters())\n",
    "            + list(self.branch3_l5.parameters())\n",
    "            + list(self.gate_l3.parameters())\n",
    "            + list(self.gate_l4.parameters())\n",
    "            + list(self.gate_l5.parameters())\n",
    "        )\n",
    "        extractor_params_ids = list(map(id, extractor_params))\n",
    "        classifier_params = filter(\n",
    "            lambda p: id(p) not in extractor_params_ids, self.parameters()\n",
    "        )\n",
    "\n",
    "        if prefix in [\"extractor\", \"extract\"]:\n",
    "            return extractor_params\n",
    "        elif prefix in [\"classifier\"]:\n",
    "            return classifier_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5aa59c0-0a12-480d-8e89-c7c2cd1b26d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3c1f6fc-b544-4d5c-a149-aa1b20ee121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/s2320437/WORK/aidan-medaf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "CURRENT_DIR = '/home/s2320437/WORK/aidan-medaf/'\n",
    "os.chdir(CURRENT_DIR)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "\n",
    "KNOWN_LABELS = [\n",
    "    \"Atelectasis\",\n",
    "    \"Cardiomegaly\",\n",
    "    \"Effusion\",\n",
    "    \"Infiltration\",\n",
    "    \"Mass\",\n",
    "    \"Nodule\",\n",
    "    \"Pneumonia\",\n",
    "    \"Pneumothorax\",\n",
    "]\n",
    "\n",
    "DEFAULT_IMAGE_ROOT = Path(f\"{CURRENT_DIR}/datasets/data/chestxray/NIH/images-224\")\n",
    "DEFAULT_KNOWN_CSV = Path(f\"{CURRENT_DIR}/datasets/data/chestxray/NIH/chestxray_train_known.csv\")\n",
    "\n",
    "DEFAULT_CHECKPOINT_DIR = Path(f\"{CURRENT_DIR}/checkpoints/medaf_phase1\")\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "results = {}\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "dataset_name = None\n",
    "class_names = KNOWN_LABELS\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29ef026e-5abb-4332-a13c-8c8a69d0db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: ChestXrayKnownDataset\n",
    "class ChestXrayKnownDataset(data.Dataset):\n",
    "    \"\"\"Dataset that reads the known-label ChestX-ray14 split for Phase 1 training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: Path,\n",
    "        image_root: Path,\n",
    "        img_size: int = 224,\n",
    "        max_samples: Optional[int] = None,\n",
    "        transform=None,\n",
    "    ) -> None:\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.image_root = Path(image_root)\n",
    "        self.img_size = img_size\n",
    "        self.class_names = KNOWN_LABELS\n",
    "        self.num_classes = len(self.class_names)\n",
    "\n",
    "\n",
    "        if not self.csv_path.exists():\n",
    "            raise FileNotFoundError(f\"ChestX-ray CSV not found: {self.csv_path}\")\n",
    "        if not self.image_root.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"ChestX-ray image directory not found: {self.image_root}\"\n",
    "            )\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((img_size, img_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225],\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        if \"known_labels\" not in df.columns:\n",
    "            raise ValueError(\n",
    "                \"Expected 'known_labels' column in CSV. Run utils/create_chestxray_splits.py first.\"\n",
    "            )\n",
    "\n",
    "        if max_samples is not None and max_samples < len(df):\n",
    "            df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "        self.records = df.to_dict(\"records\")\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.class_names)}\n",
    "        print(f\"Label to index: {self.label_to_idx}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_label_list(raw_value):\n",
    "        if isinstance(raw_value, list):\n",
    "            return raw_value\n",
    "        if pd.isna(raw_value):\n",
    "            return []\n",
    "        if isinstance(raw_value, str):\n",
    "            raw_value = raw_value.strip()\n",
    "            if not raw_value:\n",
    "                return []\n",
    "            try:\n",
    "                parsed = ast.literal_eval(raw_value)\n",
    "                if isinstance(parsed, (list, tuple)):\n",
    "                    return list(parsed)\n",
    "                if isinstance(parsed, str):\n",
    "                    return [parsed]\n",
    "            except (ValueError, SyntaxError):\n",
    "                pass\n",
    "            return [item.strip() for item in raw_value.split(\"|\") if item.strip()]\n",
    "        return []\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.records[idx]\n",
    "        image_path = self.image_root / record[\"Image Index\"]\n",
    "        if not image_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing image: {image_path}\")\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        labels = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        for label in self._parse_label_list(record.get(\"known_labels\", [])):\n",
    "            if label in self.label_to_idx:\n",
    "                labels[self.label_to_idx[label]] = 1.0\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "    # class_name\n",
    "    @staticmethod\n",
    "    def class_name():\n",
    "        return KNOWN_LABELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55d01ee3-69db-4213-bcaf-a2f3fbee62c3",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "# Demo configuration\n",
    "config = {\n",
    "    \"data_source\": \"chestxray\",\n",
    "    \"known_csv\": str(DEFAULT_KNOWN_CSV),\n",
    "    \"image_root\": str(DEFAULT_IMAGE_ROOT),\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"val_ratio\": 0.1,\n",
    "    \"num_workers\": 2,\n",
    "    # \"max_samples\": None,  # Set to an int for quicker experiments\n",
    "    \"max_samples\": 1000,\n",
    "    \"phase1_checkpoint\": \"medaf_phase1_chestxray.pt\",\n",
    "    \"checkpoint_dir\": str(DEFAULT_CHECKPOINT_DIR),\n",
    "    \"run_phase2\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38e9f237-b4aa-404f-8f5a-a3d0e71b026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ChestX-ray14 known-label split from /home/s2320437/WORK/aidan-medaf/datasets/data/chestxray/NIH/chestxray_train_known.csv\n",
      "Label to index: {'Atelectasis': 0, 'Cardiomegaly': 1, 'Effusion': 2, 'Infiltration': 3, 'Mass': 4, 'Nodule': 5, 'Pneumonia': 6, 'Pneumothorax': 7}\n",
      "Dataset prepared: 900 train / 100 val samples (ChestX-ray14 (known labels))\n"
     ]
    }
   ],
   "source": [
    "data_source = config.get(\"data_source\", \"chestxray\").lower()\n",
    "\n",
    "if data_source == \"chestxray\":\n",
    "    csv_path = Path(config.get(\"known_csv\", DEFAULT_KNOWN_CSV))\n",
    "    image_root = Path(config.get(\"image_root\", DEFAULT_IMAGE_ROOT))\n",
    "    max_samples = config.get(\"max_samples\")\n",
    "    if isinstance(max_samples, str):\n",
    "        max_samples = int(max_samples)\n",
    "    print(f\"Loading ChestX-ray14 known-label split from {csv_path}\")\n",
    "    dataset = ChestXrayKnownDataset(\n",
    "        csv_path=csv_path,\n",
    "        image_root=image_root,\n",
    "        img_size=config.get(\"img_size\", 224),\n",
    "        max_samples=max_samples,\n",
    "    )\n",
    "    dataset_name = \"ChestX-ray14 (known labels)\"\n",
    "    class_names = dataset.class_names\n",
    "    config[\"num_classes\"] = dataset.num_classes\n",
    "    config[\"img_size\"] = dataset.img_size\n",
    "\n",
    "val_ratio = float(config.get(\"val_ratio\", 0.1))\n",
    "val_size = max(1, int(len(dataset) * val_ratio))\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_dataset, val_dataset = data.random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")\n",
    "\n",
    "batch_size = config.get(\"batch_size\", 16)\n",
    "num_workers = config.get(\"num_workers\", 4)\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "test_loader = val_loader\n",
    "\n",
    "print(\n",
    "    f\"Dataset prepared: {train_size} train / {val_size} val samples ({dataset_name})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7ea132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, args, loss_history):\n",
    "    ckpt_dir = Path(config[\"checkpoint_dir\"])\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = ckpt_dir / config[\"phase1_checkpoint\"]\n",
    "\n",
    "    payload = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"args\": args,\n",
    "        \"class_names\": class_names,\n",
    "        \"dataset\": dataset_name,\n",
    "    }\n",
    "    torch.save(payload, checkpoint_path)\n",
    "\n",
    "    metadata = {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"class_names\": class_names,\n",
    "        \"num_epochs\": config[\"num_epochs\"],\n",
    "        \"batch_size\": config.get(\"batch_size\"),\n",
    "        \"learning_rate\": config.get(\"learning_rate\"),\n",
    "        \"loss_history\": [float(loss) for loss in loss_history],\n",
    "        \"device\": str(device),\n",
    "        \"checkpoint\": str(checkpoint_path),\n",
    "        \"config\": {\n",
    "            k: v\n",
    "            for k, v in config.items()\n",
    "            if isinstance(v, (int, float, str, bool))\n",
    "        },\n",
    "    }\n",
    "    metadata_path = checkpoint_path.with_suffix(\".json\")\n",
    "    with metadata_path.open(\"w\", encoding=\"utf-8\") as fp:\n",
    "        json.dump(metadata, fp, indent=2)\n",
    "\n",
    "    return checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e802e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from misc.util import *\n",
    "\n",
    "\n",
    "def multiLabelAttnDiv(cams_list, targets, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Multi-label attention diversity loss\n",
    "\n",
    "    Encourages different experts to focus on different spatial regions\n",
    "    for all positive classes in multi-label setting.\n",
    "\n",
    "    Args:\n",
    "        cams_list: List of CAMs from 3 experts [B, num_classes, H, W]\n",
    "        targets: Multi-hot labels [B, num_classes]\n",
    "        eps: Small value for numerical stability\n",
    "\n",
    "    Returns:\n",
    "        diversity_loss: Scalar tensor representing attention diversity loss\n",
    "    \"\"\"\n",
    "    if targets is None or targets.sum() == 0:\n",
    "        return torch.tensor(0.0, device=cams_list[0].device)\n",
    "\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=eps)\n",
    "    diversity_loss = 0.0\n",
    "    total_pairs = 0\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        # Get positive class indices for this sample\n",
    "        positive_classes = torch.where(targets[batch_idx] == 1)[0]\n",
    "\n",
    "        if len(positive_classes) == 0:\n",
    "            continue\n",
    "\n",
    "        # Process each positive class\n",
    "        for class_idx in positive_classes:\n",
    "            # Extract CAMs for this class from all experts\n",
    "            expert_cams = torch.stack(\n",
    "                [\n",
    "                    cams_list[0][batch_idx, class_idx],  # Expert 1: [H, W]\n",
    "                    cams_list[1][batch_idx, class_idx],  # Expert 2: [H, W]\n",
    "                    cams_list[2][batch_idx, class_idx],  # Expert 3: [H, W]\n",
    "                ]\n",
    "            )  # [3, H, W]\n",
    "\n",
    "            # Flatten spatial dimensions and normalize\n",
    "            expert_cams = expert_cams.view(3, -1)  # [3, H*W]\n",
    "            expert_cams = F.normalize(expert_cams, p=2, dim=-1)\n",
    "\n",
    "            # Remove mean activation to focus on relative attention patterns\n",
    "            mean = expert_cams.mean(dim=-1, keepdim=True)  # [3, 1]\n",
    "            expert_cams = F.relu(expert_cams - mean)\n",
    "\n",
    "            # Compute pairwise cosine similarity (encourage orthogonality)\n",
    "            for i in range(3):\n",
    "                for j in range(i + 1, 3):\n",
    "                    similarity = cos(\n",
    "                        expert_cams[i : i + 1], expert_cams[j : j + 1]\n",
    "                    ).mean()\n",
    "                    diversity_loss += similarity\n",
    "                    total_pairs += 1\n",
    "\n",
    "    # Average over all pairs\n",
    "    if total_pairs > 0:\n",
    "        return diversity_loss / total_pairs\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=cams_list[0].device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23852ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multiLabelAccuracy(predictions, targets, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute multi-label accuracy metrics\n",
    "\n",
    "    Args:\n",
    "        predictions: Model predictions [B, num_classes]\n",
    "        targets: Multi-hot ground truth [B, num_classes]\n",
    "        threshold: Threshold for binary predictions\n",
    "\n",
    "    Returns:\n",
    "        subset_acc: Exact match accuracy (all labels correct)\n",
    "        hamming_acc: Label-wise accuracy\n",
    "        precision: Precision score\n",
    "        recall: Recall score\n",
    "        f1: F1 score\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Convert logits to probabilities and then to binary predictions\n",
    "        probs = torch.sigmoid(predictions)\n",
    "        pred_binary = (probs > threshold).float()\n",
    "\n",
    "        # Subset accuracy (exact match)\n",
    "        subset_acc = (pred_binary == targets).all(dim=1).float().mean()\n",
    "\n",
    "        # Hamming accuracy (label-wise accuracy)\n",
    "        hamming_acc = (pred_binary == targets).float().mean()\n",
    "\n",
    "        # Precision, Recall, F1\n",
    "        tp = (pred_binary * targets).sum(dim=0)\n",
    "        fp = (pred_binary * (1 - targets)).sum(dim=0)\n",
    "        fn = ((1 - pred_binary) * targets).sum(dim=0)\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-8)\n",
    "        recall = tp / (tp + fn + 1e-8)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "        # Average across classes\n",
    "        precision = precision.mean()\n",
    "        recall = recall.mean()\n",
    "        f1 = f1.mean()\n",
    "\n",
    "    return subset_acc, hamming_acc, precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caa29c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_multilabel(train_loader, model, criterion, optimizer, args, device=None):\n",
    "    \"\"\"\n",
    "    Training loop for multi-label MEDAF\n",
    "\n",
    "    Args:\n",
    "        train_loader: DataLoader with multi-label data\n",
    "        model: MultiLabelMEDAF model\n",
    "        criterion: Dictionary containing loss functions\n",
    "        optimizer: Optimizer\n",
    "        args: Training arguments\n",
    "        device: Device to run on\n",
    "\n",
    "    Returns:\n",
    "        Average training loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    loss_keys = args[\"loss_keys\"]  # [\"b1\", \"b2\", \"b3\", \"gate\", \"divAttn\", \"total\"]\n",
    "    acc_keys = args[\"acc_keys\"]  # [\"acc1\", \"acc2\", \"acc3\", \"accGate\"]\n",
    "\n",
    "    loss_meter = {p: AverageMeter() for p in loss_keys}\n",
    "    acc_meter = {p: AverageMeter() for p in acc_keys}\n",
    "    time_start = time.time()\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs = data[0].to(device)\n",
    "        targets = data[1].to(device)  # Multi-hot labels [B, num_classes]\n",
    "\n",
    "        # Forward pass\n",
    "        output_dict = model(inputs, targets)\n",
    "        logits = output_dict[\"logits\"]  # List of logits from 4 heads\n",
    "        cams_list = output_dict[\"cams_list\"]  # CAMs from 3 experts\n",
    "\n",
    "        # Multi-label classification losses for expert branches\n",
    "        bce_losses = [\n",
    "            criterion[\"bce\"](logit.float(), targets.float())\n",
    "            for logit in logits[:3]  # Expert branches only\n",
    "        ]\n",
    "\n",
    "        # Gating loss (on fused predictions)\n",
    "        gate_loss = criterion[\"bce\"](logits[3].float(), targets.float())\n",
    "\n",
    "        # Multi-label attention diversity loss\n",
    "        diversity_loss = multiLabelAttnDiv(cams_list, targets)\n",
    "\n",
    "        # Combine losses according to weights\n",
    "        loss_values = bce_losses + [gate_loss, diversity_loss]\n",
    "        total_loss = (\n",
    "            args[\"loss_wgts\"][0] * sum(bce_losses)  # Expert loss weight\n",
    "            + args[\"loss_wgts\"][1] * gate_loss  # Gating loss weight\n",
    "            + args[\"loss_wgts\"][2] * diversity_loss  # Diversity loss weight\n",
    "        )\n",
    "        loss_values.append(total_loss)\n",
    "\n",
    "        # Compute multi-label accuracies\n",
    "        acc_values = []\n",
    "        for logit in logits:\n",
    "            subset_acc, hamming_acc, _, _, _ = multiLabelAccuracy(logit, targets)\n",
    "            acc_values.append(subset_acc * 100)  # Convert to percentage\n",
    "\n",
    "        # Update meters\n",
    "        multi_loss = {loss_keys[k]: loss_values[k] for k in range(len(loss_keys))}\n",
    "        train_accs = {acc_keys[k]: acc_values[k] for k in range(len(acc_keys))}\n",
    "\n",
    "        update_meter(loss_meter, multi_loss, inputs.size(0))\n",
    "        update_meter(acc_meter, train_accs, inputs.size(0))\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if i % 50 == 0:  # Print every 50 batches\n",
    "            tmp_str = f\"Batch [{i}/{len(train_loader)}] \"\n",
    "            tmp_str += \"< Training Loss >\\n\"\n",
    "            for k, v in loss_meter.items():\n",
    "                tmp_str += f\"{k}:{v.value:.4f} \"\n",
    "            tmp_str += \"\\n< Training Accuracy >\\n\"\n",
    "            for k, v in acc_meter.items():\n",
    "                tmp_str += f\"{k}:{v.value:.1f} \"\n",
    "            print(tmp_str)\n",
    "\n",
    "    time_elapsed = time.time() - time_start\n",
    "    print(f\"\\nEpoch completed in {time_elapsed:.1f}s\")\n",
    "\n",
    "    # Final epoch summary\n",
    "    tmp_str = \"< Final Training Loss >\\n\"\n",
    "    for k, v in loss_meter.items():\n",
    "        tmp_str += f\"{k}:{v.value:.4f} \"\n",
    "    tmp_str += \"\\n< Final Training Accuracy >\\n\"\n",
    "    for k, v in acc_meter.items():\n",
    "        tmp_str += f\"{k}:{v.value:.1f} \"\n",
    "    print(tmp_str)\n",
    "\n",
    "    return loss_meter[loss_keys[-1]].value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8455a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0c073d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 1: Basic Multi-Label MEDAF\n",
      "============================================================\n",
      "Making resnet layer with channel 64 block 2 stride 1\n",
      "Making resnet layer with channel 128 block 2 stride 2\n",
      "Making resnet layer with channel 256 block 2 stride 2\n",
      "Making resnet layer with channel 512 block 2 stride 2\n",
      "Phase 1 Model Parameters: 44,749,955\n",
      "Batch [0/29] < Training Loss >\n",
      "b1:0.8801 b2:0.7273 b3:0.6110 gate:0.7187 divAttn:0.3237 total:2.2748 \n",
      "< Training Accuracy >\n",
      "acc1:0.0 acc2:0.0 acc3:0.0 accGate:0.0 \n",
      "\n",
      "Epoch completed in 37.8s\n",
      "< Final Training Loss >\n",
      "b1:0.3716 b2:0.3237 b3:0.2963 gate:0.3232 divAttn:0.4948 total:1.0223 \n",
      "< Final Training Accuracy >\n",
      "acc1:51.7 acc2:52.6 acc3:55.6 accGate:54.9 \n",
      "Epoch 0: Loss=1.0223\n",
      "Batch [0/29] < Training Loss >\n",
      "b1:0.2130 b2:0.1943 b3:0.1901 gate:0.1968 divAttn:0.3644 total:0.6186 \n",
      "< Training Accuracy >\n",
      "acc1:68.8 acc2:68.8 acc3:68.8 accGate:68.8 \n",
      "\n",
      "Epoch completed in 33.6s\n",
      "< Final Training Loss >\n",
      "b1:0.2495 b2:0.2437 b3:0.2428 gate:0.2440 divAttn:0.2883 total:0.7621 \n",
      "< Final Training Accuracy >\n",
      "acc1:58.3 acc2:58.6 acc3:58.4 accGate:58.3 \n",
      "Batch [0/29] < Training Loss >\n",
      "b1:0.2447 b2:0.2393 b3:0.2427 gate:0.2413 divAttn:0.2652 total:0.7526 \n",
      "< Training Accuracy >\n",
      "acc1:53.1 acc2:56.2 acc3:56.2 accGate:56.2 \n",
      "\n",
      "Epoch completed in 33.6s\n",
      "< Final Training Loss >\n",
      "b1:0.2419 b2:0.2400 b3:0.2389 gate:0.2394 divAttn:0.1997 total:0.7459 \n",
      "< Final Training Accuracy >\n",
      "acc1:58.2 acc2:58.4 acc3:58.6 accGate:58.6 \n",
      "Epoch 2: Loss=0.7459\n",
      "Batch [0/29] < Training Loss >\n",
      "b1:0.2478 b2:0.2417 b3:0.2396 gate:0.2420 divAttn:0.2186 total:0.7546 \n",
      "< Training Accuracy >\n",
      "acc1:59.4 acc2:59.4 acc3:59.4 accGate:59.4 \n",
      "\n",
      "Epoch completed in 33.6s\n",
      "< Final Training Loss >\n",
      "b1:0.2404 b2:0.2385 b3:0.2386 gate:0.2385 divAttn:0.1656 total:0.7424 \n",
      "< Final Training Accuracy >\n",
      "acc1:58.6 acc2:58.7 acc3:58.7 accGate:58.6 \n",
      "Batch [0/29] < Training Loss >\n",
      "b1:0.2441 b2:0.2480 b3:0.2434 gate:0.2443 divAttn:0.1440 total:0.7606 \n",
      "< Training Accuracy >\n",
      "acc1:59.4 acc2:59.4 acc3:59.4 accGate:59.4 \n",
      "\n",
      "Epoch completed in 33.6s\n",
      "< Final Training Loss >\n",
      "b1:0.2406 b2:0.2409 b3:0.2388 gate:0.2391 divAttn:0.1814 total:0.7451 \n",
      "< Final Training Accuracy >\n",
      "acc1:58.6 acc2:58.8 acc3:58.4 accGate:58.6 \n",
      "Epoch 4: Loss=0.7451\n",
      "Phase 1 Final Loss: 0.7451\n",
      "Phase 1 checkpoint saved to: /home/s2320437/WORK/aidan-medaf/checkpoints/medaf_phase1/medaf_phase1_chestxray.pt\n",
      "Use load_phase1_checkpoint(CheckpointPath) to reload this model for evaluation.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Demonstrate Phase 1: Basic Multi-Label MEDAF\"\"\"\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 1: Basic Multi-Label MEDAF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration for Phase 1\n",
    "args = {\n",
    "    \"img_size\": config[\"img_size\"],\n",
    "    \"backbone\": \"resnet18\",\n",
    "    \"num_classes\": config[\"num_classes\"],\n",
    "    \"gate_temp\": 100,\n",
    "    \"loss_keys\": [\"b1\", \"b2\", \"b3\", \"gate\", \"divAttn\", \"total\"],\n",
    "    \"acc_keys\": [\"acc1\", \"acc2\", \"acc3\", \"accGate\"],\n",
    "    \"loss_wgts\": [0.7, 1.0, 0.01],\n",
    "}\n",
    "\n",
    "# Create Phase 1 model\n",
    "model = MultiLabelMEDAF(args)\n",
    "model.to(device)\n",
    "\n",
    "print(\n",
    "    f\"Phase 1 Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\"\n",
    ")\n",
    "\n",
    "# Training setup\n",
    "criterion = {\"bce\": nn.BCEWithLogitsLoss()}\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=config[\"learning_rate\"]\n",
    ")\n",
    "\n",
    "# Training\n",
    "phase1_metrics = []\n",
    "for epoch in range(config[\"num_epochs\"]):\n",
    "    metrics = train_multilabel(\n",
    "        train_loader, model, criterion, optimizer, args, device\n",
    "    )\n",
    "    phase1_metrics.append(metrics)\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss={metrics:.4f}\")\n",
    "\n",
    "final_loss = phase1_metrics[-1] if phase1_metrics else float(\"nan\")\n",
    "checkpoint_path = save_model(model, args, phase1_metrics)\n",
    "\n",
    "results[\"phase1\"] = {\n",
    "    \"model\": model,\n",
    "    \"final_loss\": final_loss,\n",
    "    \"metrics_history\": phase1_metrics,\n",
    "    \"checkpoint\": str(checkpoint_path),\n",
    "}\n",
    "\n",
    "if phase1_metrics:\n",
    "    print(f\"Phase 1 Final Loss: {final_loss:.4f}\")\n",
    "else:\n",
    "    print(\"Phase 1 completed with zero epochs (no training performed)\")\n",
    "print(f\"Phase 1 checkpoint saved to: {checkpoint_path}\")\n",
    "print(\n",
    "    \"Use load_phase1_checkpoint(CheckpointPath) to reload this model for evaluation.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research_medaf_aidan)",
   "language": "python",
   "name": "research_medaf_aidan"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
