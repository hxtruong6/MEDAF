{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5aa59c0-0a12-480d-8e89-c7c2cd1b26d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e549a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Memory optimizations applied\n"
     ]
    }
   ],
   "source": [
    "# ===== MEMORY OPTIMIZATION =====\n",
    "\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "gc.collect()\n",
    "\n",
    "# Set memory management\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Reduce memory usage\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_per_process_memory_fraction(0.9)\n",
    "\n",
    "print(\"✅ Memory optimizations applied\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c1f6fc-b544-4d5c-a149-aa1b20ee121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/s2320437/WORK/aidan-medaf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "CURRENT_DIR = '/home/s2320437/WORK/aidan-medaf/'\n",
    "os.chdir(CURRENT_DIR)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "\n",
    "KNOWN_LABELS = [\n",
    "    \"Atelectasis\",\n",
    "    \"Cardiomegaly\",\n",
    "    \"Effusion\",\n",
    "    \"Infiltration\",\n",
    "    \"Mass\",\n",
    "    \"Nodule\",\n",
    "    \"Pneumonia\",\n",
    "    \"Pneumothorax\",\n",
    "]\n",
    "\n",
    "DEFAULT_IMAGE_ROOT = Path(f\"{CURRENT_DIR}/datasets/data/chestxray/NIH/images-224\")\n",
    "DEFAULT_KNOWN_CSV = Path(f\"{CURRENT_DIR}/datasets/data/chestxray/NIH/chestxray_train_known.csv\")\n",
    "\n",
    "DEFAULT_CHECKPOINT_DIR = Path(f\"{CURRENT_DIR}/checkpoints/medaf_phase1\")\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "results = {}\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "dataset_name = None\n",
    "class_names = KNOWN_LABELS\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a2bca86-a8bf-4528-b917-1d9a07411aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from core.net import build_backbone, conv1x1, Classifier\n",
    "\n",
    "class MultiLabelMEDAF(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Label version of MEDAF (Multi-Expert Diverse Attention Fusion)\n",
    "\n",
    "    Key changes from original MEDAF:\n",
    "    1. Support for multi-hot label targets\n",
    "    2. BCEWithLogitsLoss instead of CrossEntropyLoss\n",
    "    3. Multi-label attention diversity computation\n",
    "    4. Per-sample CAM extraction for multiple positive classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args=None):\n",
    "        super(MultiLabelMEDAF, self).__init__()\n",
    "        backbone, feature_dim, self.cam_size = build_backbone(\n",
    "            img_size=args[\"img_size\"],\n",
    "            backbone_name=args[\"backbone\"],\n",
    "            projection_dim=-1,\n",
    "            inchan=3,\n",
    "        )\n",
    "        self.img_size = args[\"img_size\"]\n",
    "        self.gate_temp = args[\"gate_temp\"]\n",
    "        self.num_classes = args[\n",
    "            \"num_classes\"\n",
    "        ]  # Changed from num_known to num_classes for clarity\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Shared layers (L1-L3)\n",
    "        self.shared_l3 = nn.Sequential(*list(backbone.children())[:-6])\n",
    "\n",
    "        # Expert branch 1\n",
    "        self.branch1_l4 = nn.Sequential(*list(backbone.children())[-6:-3])\n",
    "        self.branch1_l5 = nn.Sequential(*list(backbone.children())[-3])\n",
    "        self.branch1_cls = conv1x1(feature_dim, self.num_classes)\n",
    "\n",
    "        # Expert branch 2 (deep copy)\n",
    "        self.branch2_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.branch2_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        self.branch2_cls = conv1x1(feature_dim, self.num_classes)\n",
    "\n",
    "        # Expert branch 3 (deep copy)\n",
    "        self.branch3_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.branch3_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        self.branch3_cls = conv1x1(feature_dim, self.num_classes)\n",
    "\n",
    "        # Gating network\n",
    "        self.gate_l3 = copy.deepcopy(self.shared_l3)\n",
    "        self.gate_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.gate_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        self.gate_cls = nn.Sequential(\n",
    "            Classifier(feature_dim, int(feature_dim / 4), bias=True),\n",
    "            Classifier(int(feature_dim / 4), 3, bias=True),  # 3 experts\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y=None, return_ft=False):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-label MEDAF\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor [B, C, H, W]\n",
    "            y: Multi-hot labels [B, num_classes] or None\n",
    "            return_ft: Whether to return features\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing logits, gate predictions, and CAMs/features\n",
    "        \"\"\"\n",
    "        b = x.size(0)\n",
    "        ft_till_l3 = self.shared_l3(x)\n",
    "\n",
    "        # Expert branch 1\n",
    "        branch1_l4 = self.branch1_l4(ft_till_l3.clone())\n",
    "        branch1_l5 = self.branch1_l5(branch1_l4)\n",
    "        b1_ft_cams = self.branch1_cls(branch1_l5)  # [B, num_classes, H, W]\n",
    "        b1_logits = self.avg_pool(b1_ft_cams).view(b, -1)\n",
    "\n",
    "        # Expert branch 2\n",
    "        branch2_l4 = self.branch2_l4(ft_till_l3.clone())\n",
    "        branch2_l5 = self.branch2_l5(branch2_l4)\n",
    "        b2_ft_cams = self.branch2_cls(branch2_l5)  # [B, num_classes, H, W]\n",
    "        b2_logits = self.avg_pool(b2_ft_cams).view(b, -1)\n",
    "\n",
    "        # Expert branch 3\n",
    "        branch3_l4 = self.branch3_l4(ft_till_l3.clone())\n",
    "        branch3_l5 = self.branch3_l5(branch3_l4)\n",
    "        b3_ft_cams = self.branch3_cls(branch3_l5)  # [B, num_classes, H, W]\n",
    "        b3_logits = self.avg_pool(b3_ft_cams).view(b, -1)\n",
    "\n",
    "        # Store CAMs for diversity loss computation\n",
    "        cams_list = [b1_ft_cams, b2_ft_cams, b3_ft_cams]\n",
    "\n",
    "        # Multi-label CAM extraction for positive classes\n",
    "        if y is not None:\n",
    "            # Extract CAMs for all positive classes across all experts\n",
    "            # This will be used for attention diversity computation\n",
    "            multi_label_cams = self._extract_multilabel_cams(cams_list, y)\n",
    "        else:\n",
    "            multi_label_cams = None\n",
    "\n",
    "        if return_ft:\n",
    "            # Aggregate features from all experts\n",
    "            fts = (\n",
    "                b1_ft_cams.detach().clone()\n",
    "                + b2_ft_cams.detach().clone()\n",
    "                + b3_ft_cams.detach().clone()\n",
    "            )\n",
    "\n",
    "        # Gating network\n",
    "        gate_l5 = self.gate_l5(self.gate_l4(self.gate_l3(x)))\n",
    "        gate_pool = self.avg_pool(gate_l5).view(b, -1)\n",
    "        gate_pred = F.softmax(self.gate_cls(gate_pool) / self.gate_temp, dim=1)\n",
    "\n",
    "        # Adaptive fusion using gating weights\n",
    "        gate_logits = torch.stack(\n",
    "            [b1_logits.detach(), b2_logits.detach(), b3_logits.detach()], dim=-1\n",
    "        )\n",
    "        gate_logits = gate_logits * gate_pred.view(\n",
    "            gate_pred.size(0), 1, gate_pred.size(1)\n",
    "        )\n",
    "        gate_logits = gate_logits.sum(-1)\n",
    "\n",
    "        logits_list = [b1_logits, b2_logits, b3_logits, gate_logits]\n",
    "\n",
    "        if return_ft and y is None:\n",
    "            outputs = {\n",
    "                \"logits\": logits_list,\n",
    "                \"gate_pred\": gate_pred,\n",
    "                \"fts\": fts,\n",
    "                \"cams_list\": cams_list,\n",
    "            }\n",
    "        else:\n",
    "            outputs = {\n",
    "                \"logits\": logits_list,\n",
    "                \"gate_pred\": gate_pred,\n",
    "                \"multi_label_cams\": multi_label_cams,\n",
    "                \"cams_list\": cams_list,\n",
    "            }\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _extract_multilabel_cams(self, cams_list, targets):\n",
    "        \"\"\"\n",
    "        Extract CAMs for all positive classes in multi-label setting\n",
    "\n",
    "        Args:\n",
    "            cams_list: List of CAMs from 3 experts [B, num_classes, H, W]\n",
    "            targets: Multi-hot labels [B, num_classes]\n",
    "\n",
    "        Returns:\n",
    "            extracted_cams: List of CAMs for positive classes per expert\n",
    "        \"\"\"\n",
    "        batch_size = targets.size(0)\n",
    "        extracted_cams = []\n",
    "\n",
    "        for expert_idx, expert_cams in enumerate(cams_list):\n",
    "            expert_extracted = []\n",
    "\n",
    "            for batch_idx in range(batch_size):\n",
    "                # Find positive class indices for this sample\n",
    "                positive_classes = torch.where(targets[batch_idx] == 1)[0]\n",
    "\n",
    "                if len(positive_classes) > 0:\n",
    "                    # Extract CAMs for positive classes\n",
    "                    sample_cams = expert_cams[\n",
    "                        batch_idx, positive_classes\n",
    "                    ]  # [num_positive, H, W]\n",
    "                    expert_extracted.append(sample_cams)\n",
    "                else:\n",
    "                    # If no positive classes, create zero tensor\n",
    "                    H, W = expert_cams.shape[-2:]\n",
    "                    expert_extracted.append(\n",
    "                        torch.zeros(1, H, W, device=expert_cams.device)\n",
    "                    )\n",
    "\n",
    "            extracted_cams.append(expert_extracted)\n",
    "\n",
    "        return extracted_cams\n",
    "\n",
    "    def get_params(self, prefix=\"extractor\"):\n",
    "        \"\"\"Get model parameters for different learning rates\"\"\"\n",
    "        extractor_params = (\n",
    "            list(self.shared_l3.parameters())\n",
    "            + list(self.branch1_l4.parameters())\n",
    "            + list(self.branch1_l5.parameters())\n",
    "            + list(self.branch2_l4.parameters())\n",
    "            + list(self.branch2_l5.parameters())\n",
    "            + list(self.branch3_l4.parameters())\n",
    "            + list(self.branch3_l5.parameters())\n",
    "            + list(self.gate_l3.parameters())\n",
    "            + list(self.gate_l4.parameters())\n",
    "            + list(self.gate_l5.parameters())\n",
    "        )\n",
    "        extractor_params_ids = list(map(id, extractor_params))\n",
    "        classifier_params = filter(\n",
    "            lambda p: id(p) not in extractor_params_ids, self.parameters()\n",
    "        )\n",
    "\n",
    "        if prefix in [\"extractor\", \"extract\"]:\n",
    "            return extractor_params\n",
    "        elif prefix in [\"classifier\"]:\n",
    "            return classifier_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ef026e-5abb-4332-a13c-8c8a69d0db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: ChestXrayKnownDataset\n",
    "class ChestXrayKnownDataset(data.Dataset):\n",
    "    \"\"\"Dataset that reads the known-label ChestX-ray14 split for Phase 1 training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: Path,\n",
    "        image_root: Path,\n",
    "        img_size: int = 224,\n",
    "        max_samples: Optional[int] = None,\n",
    "        transform=None,\n",
    "    ) -> None:\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.image_root = Path(image_root)\n",
    "        self.img_size = img_size\n",
    "        self.class_names = KNOWN_LABELS\n",
    "        self.num_classes = len(self.class_names)\n",
    "\n",
    "\n",
    "        if not self.csv_path.exists():\n",
    "            raise FileNotFoundError(f\"ChestX-ray CSV not found: {self.csv_path}\")\n",
    "        if not self.image_root.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"ChestX-ray image directory not found: {self.image_root}\"\n",
    "            )\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((img_size, img_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225],\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        if \"known_labels\" not in df.columns:\n",
    "            raise ValueError(\n",
    "                \"Expected 'known_labels' column in CSV. Run utils/create_chestxray_splits.py first.\"\n",
    "            )\n",
    "\n",
    "        if max_samples is not None and max_samples < len(df):\n",
    "            df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "        self.records = df.to_dict(\"records\")\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.class_names)}\n",
    "        print(f\"Label to index: {self.label_to_idx}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_label_list(raw_value):\n",
    "        if isinstance(raw_value, list):\n",
    "            return raw_value\n",
    "        if pd.isna(raw_value):\n",
    "            return []\n",
    "        if isinstance(raw_value, str):\n",
    "            raw_value = raw_value.strip()\n",
    "            if not raw_value:\n",
    "                return []\n",
    "            try:\n",
    "                parsed = ast.literal_eval(raw_value)\n",
    "                if isinstance(parsed, (list, tuple)):\n",
    "                    return list(parsed)\n",
    "                if isinstance(parsed, str):\n",
    "                    return [parsed]\n",
    "            except (ValueError, SyntaxError):\n",
    "                pass\n",
    "            return [item.strip() for item in raw_value.split(\"|\") if item.strip()]\n",
    "        return []\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.records[idx]\n",
    "        image_path = self.image_root / record[\"Image Index\"]\n",
    "        if not image_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing image: {image_path}\")\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        labels = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        for label in self._parse_label_list(record.get(\"known_labels\", [])):\n",
    "            if label in self.label_to_idx:\n",
    "                labels[self.label_to_idx[label]] = 1.0\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "    # class_name\n",
    "    @staticmethod\n",
    "    def class_name():\n",
    "        return KNOWN_LABELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55d01ee3-69db-4213-bcaf-a2f3fbee62c3",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "# Demo configuration\n",
    "config = {\n",
    "    \"data_source\": \"chestxray\",\n",
    "    \"known_csv\": str(DEFAULT_KNOWN_CSV),\n",
    "    \"image_root\": str(DEFAULT_IMAGE_ROOT),\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"val_ratio\": 0.1,\n",
    "    \"num_workers\": 1,\n",
    "    # \"max_samples\": None,  # Set to an int for quicker experiments\n",
    "    # \"max_samples\": 1000,\n",
    "    \"phase1_checkpoint\": \"medaf_phase1_chestxray.pt\",\n",
    "    \"checkpoint_dir\": str(DEFAULT_CHECKPOINT_DIR),\n",
    "    \"run_phase2\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e9f237-b4aa-404f-8f5a-a3d0e71b026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ChestX-ray14 known-label split from /home/s2320437/WORK/aidan-medaf/datasets/data/chestxray/NIH/chestxray_train_known.csv\n",
      "Label to index: {'Atelectasis': 0, 'Cardiomegaly': 1, 'Effusion': 2, 'Infiltration': 3, 'Mass': 4, 'Nodule': 5, 'Pneumonia': 6, 'Pneumothorax': 7}\n",
      "Dataset prepared: 96294 train / 10699 val samples (ChestX-ray14 (known labels))\n"
     ]
    }
   ],
   "source": [
    "data_source = config.get(\"data_source\", \"chestxray\").lower()\n",
    "\n",
    "if data_source == \"chestxray\":\n",
    "    csv_path = Path(config.get(\"known_csv\", DEFAULT_KNOWN_CSV))\n",
    "    image_root = Path(config.get(\"image_root\", DEFAULT_IMAGE_ROOT))\n",
    "    max_samples = config.get(\"max_samples\")\n",
    "    if isinstance(max_samples, str):\n",
    "        max_samples = int(max_samples)\n",
    "    print(f\"Loading ChestX-ray14 known-label split from {csv_path}\")\n",
    "    dataset = ChestXrayKnownDataset(\n",
    "        csv_path=csv_path,\n",
    "        image_root=image_root,\n",
    "        img_size=config.get(\"img_size\", 224),\n",
    "        max_samples=max_samples,\n",
    "    )\n",
    "    dataset_name = \"ChestX-ray14 (known labels)\"\n",
    "    class_names = dataset.class_names\n",
    "    config[\"num_classes\"] = dataset.num_classes\n",
    "    config[\"img_size\"] = dataset.img_size\n",
    "\n",
    "val_ratio = float(config.get(\"val_ratio\", 0.1))\n",
    "val_size = max(1, int(len(dataset) * val_ratio))\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_dataset, val_dataset = data.random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")\n",
    "\n",
    "batch_size = config.get(\"batch_size\", 16)\n",
    "num_workers = config.get(\"num_workers\", 4)\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "test_loader = val_loader\n",
    "\n",
    "print(\n",
    "    f\"Dataset prepared: {train_size} train / {val_size} val samples ({dataset_name})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7ea132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, args, loss_history):\n",
    "    ckpt_dir = Path(config[\"checkpoint_dir\"])\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = ckpt_dir / config[\"phase1_checkpoint\"]\n",
    "\n",
    "    payload = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"args\": args,\n",
    "        \"class_names\": class_names,\n",
    "        \"dataset\": dataset_name,\n",
    "    }\n",
    "    torch.save(payload, checkpoint_path)\n",
    "\n",
    "    metadata = {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"class_names\": class_names,\n",
    "        \"num_epochs\": config[\"num_epochs\"],\n",
    "        \"batch_size\": config.get(\"batch_size\"),\n",
    "        \"learning_rate\": config.get(\"learning_rate\"),\n",
    "        \"loss_history\": [float(loss) for loss in loss_history],\n",
    "        \"device\": str(device),\n",
    "        \"checkpoint\": str(checkpoint_path),\n",
    "        \"config\": {\n",
    "            k: v\n",
    "            for k, v in config.items()\n",
    "            if isinstance(v, (int, float, str, bool))\n",
    "        },\n",
    "    }\n",
    "    metadata_path = checkpoint_path.with_suffix(\".json\")\n",
    "    with metadata_path.open(\"w\", encoding=\"utf-8\") as fp:\n",
    "        json.dump(metadata, fp, indent=2)\n",
    "\n",
    "    return checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e802e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from misc.util import *\n",
    "\n",
    "\n",
    "def multiLabelAttnDiv(cams_list, targets, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Multi-label attention diversity loss\n",
    "\n",
    "    Encourages different experts to focus on different spatial regions\n",
    "    for all positive classes in multi-label setting.\n",
    "\n",
    "    Args:\n",
    "        cams_list: List of CAMs from 3 experts [B, num_classes, H, W]\n",
    "        targets: Multi-hot labels [B, num_classes]\n",
    "        eps: Small value for numerical stability\n",
    "\n",
    "    Returns:\n",
    "        diversity_loss: Scalar tensor representing attention diversity loss\n",
    "    \"\"\"\n",
    "    if targets is None or targets.sum() == 0:\n",
    "        return torch.tensor(0.0, device=cams_list[0].device)\n",
    "\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=eps)\n",
    "    diversity_loss = 0.0\n",
    "    total_pairs = 0\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        # Get positive class indices for this sample\n",
    "        positive_classes = torch.where(targets[batch_idx] == 1)[0]\n",
    "\n",
    "        if len(positive_classes) == 0:\n",
    "            continue\n",
    "\n",
    "        # Process each positive class\n",
    "        for class_idx in positive_classes:\n",
    "            # Extract CAMs for this class from all experts\n",
    "            expert_cams = torch.stack(\n",
    "                [\n",
    "                    cams_list[0][batch_idx, class_idx],  # Expert 1: [H, W]\n",
    "                    cams_list[1][batch_idx, class_idx],  # Expert 2: [H, W]\n",
    "                    cams_list[2][batch_idx, class_idx],  # Expert 3: [H, W]\n",
    "                ]\n",
    "            )  # [3, H, W]\n",
    "\n",
    "            # Flatten spatial dimensions and normalize\n",
    "            expert_cams = expert_cams.view(3, -1)  # [3, H*W]\n",
    "            expert_cams = F.normalize(expert_cams, p=2, dim=-1)\n",
    "\n",
    "            # Remove mean activation to focus on relative attention patterns\n",
    "            mean = expert_cams.mean(dim=-1, keepdim=True)  # [3, 1]\n",
    "            expert_cams = F.relu(expert_cams - mean)\n",
    "\n",
    "            # Compute pairwise cosine similarity (encourage orthogonality)\n",
    "            for i in range(3):\n",
    "                for j in range(i + 1, 3):\n",
    "                    similarity = cos(\n",
    "                        expert_cams[i : i + 1], expert_cams[j : j + 1]\n",
    "                    ).mean()\n",
    "                    diversity_loss += similarity\n",
    "                    total_pairs += 1\n",
    "\n",
    "    # Average over all pairs\n",
    "    if total_pairs > 0:\n",
    "        return diversity_loss / total_pairs\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=cams_list[0].device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23852ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multiLabelAccuracy(predictions, targets, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute multi-label accuracy metrics for MEDAF model evaluation\n",
    "    \n",
    "    This function calculates different types of accuracy for multi-label classification:\n",
    "    - Subset Accuracy: Strictest metric - ALL predicted labels must match ground truth\n",
    "    - Hamming Accuracy: More lenient - average accuracy across all label predictions\n",
    "    - Precision/Recall/F1: Standard classification metrics averaged across classes\n",
    "\n",
    "    Args:\n",
    "        predictions: Model predictions [B, num_classes] (logits from expert/gate)\n",
    "        targets: Multi-hot ground truth [B, num_classes] (0/1 for each disease class)\n",
    "        threshold: Threshold for converting probabilities to binary predictions (default: 0.5)\n",
    "\n",
    "    Returns:\n",
    "        subset_acc: Exact match accuracy (all labels correct) - used in training output\n",
    "        hamming_acc: Label-wise accuracy (not used in training output)\n",
    "        precision: Precision score (not used in training output)\n",
    "        recall: Recall score (not used in training output)\n",
    "        f1: F1 score (not used in training output)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Convert logits to probabilities using sigmoid (multi-label setting)\n",
    "        probs = torch.sigmoid(predictions)\n",
    "        pred_binary = (probs > threshold).float()  # Convert to binary predictions\n",
    "\n",
    "        # Subset accuracy (exact match) - STRICTEST METRIC\n",
    "        # A sample is correct only if ALL predicted labels match ground truth exactly\n",
    "        # Example: If ground truth is [1,0,1,0] and prediction is [1,0,1,0] → correct\n",
    "        #          If ground truth is [1,0,1,0] and prediction is [1,0,0,0] → incorrect\n",
    "        subset_acc = (pred_binary == targets).all(dim=1).float().mean()\n",
    "\n",
    "        # Hamming accuracy (label-wise accuracy) - MORE LENIENT\n",
    "        # Average accuracy across all individual label predictions\n",
    "        # Example: If ground truth is [1,0,1,0] and prediction is [1,0,0,0] → 3/4 = 75%\n",
    "        hamming_acc = (pred_binary == targets).float().mean()\n",
    "\n",
    "        # Precision, Recall, F1 - Standard classification metrics\n",
    "        tp = (pred_binary * targets).sum(dim=0)      # True positives per class\n",
    "        fp = (pred_binary * (1 - targets)).sum(dim=0)  # False positives per class\n",
    "        fn = ((1 - pred_binary) * targets).sum(dim=0)  # False negatives per class\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-8)  # Precision per class\n",
    "        recall = tp / (tp + fn + 1e-8)     # Recall per class\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-8)  # F1 per class\n",
    "\n",
    "        # Average across all classes\n",
    "        precision = precision.mean()\n",
    "        recall = recall.mean()\n",
    "        f1 = f1.mean()\n",
    "\n",
    "    return subset_acc, hamming_acc, precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caa29c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_multilabel(train_loader, model, criterion, optimizer, args, device=None):\n",
    "    \"\"\"\n",
    "    Training loop for multi-label MEDAF (Multi-Expert Diverse Attention Fusion)\n",
    "    \n",
    "    This function trains a multi-expert model where:\n",
    "    - 3 Expert branches (b1, b2, b3) learn different representations\n",
    "    - 1 Gating network adaptively fuses expert predictions\n",
    "    - Attention diversity loss encourages experts to focus on different regions\n",
    "\n",
    "    Args:\n",
    "        train_loader: DataLoader with multi-label data\n",
    "        model: MultiLabelMEDAF model\n",
    "        criterion: Dictionary containing loss functions\n",
    "        optimizer: Optimizer\n",
    "        args: Training arguments\n",
    "        device: Device to run on\n",
    "\n",
    "    Returns:\n",
    "        Average training loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # Loss tracking keys: [\"b1\", \"b2\", \"b3\", \"gate\", \"divAttn\", \"total\"]\n",
    "    # - b1, b2, b3: Individual expert branch losses (BCE for multi-label classification)\n",
    "    # - gate: Gating network loss (BCE for fused predictions)\n",
    "    # - divAttn: Attention diversity loss (encourages different spatial attention patterns)\n",
    "    # - total: Weighted combination of all losses\n",
    "    loss_keys = args[\"loss_keys\"]\n",
    "    \n",
    "    # Accuracy tracking keys: [\"acc1\", \"acc2\", \"acc3\", \"accGate\"]\n",
    "    # - acc1, acc2, acc3: Subset accuracy for each expert branch (exact label match)\n",
    "    # - accGate: Subset accuracy for gating network (fused predictions)\n",
    "    acc_keys = args[\"acc_keys\"]\n",
    "\n",
    "    loss_meter = {p: AverageMeter() for p in loss_keys}\n",
    "    acc_meter = {p: AverageMeter() for p in acc_keys}\n",
    "    time_start = time.time()\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs = data[0].to(device)\n",
    "        targets = data[1].to(device)  # Multi-hot labels [B, num_classes]\n",
    "\n",
    "        # Forward pass through MEDAF model\n",
    "        output_dict = model(inputs, targets)\n",
    "        logits = output_dict[\"logits\"]  # List of logits from 4 heads: [b1, b2, b3, gate]\n",
    "        cams_list = output_dict[\"cams_list\"]  # CAMs from 3 experts for attention diversity\n",
    "\n",
    "        # ===== LOSS COMPUTATION =====\n",
    "        \n",
    "        # Expert branch losses (b1, b2, b3): BCE loss for multi-label classification\n",
    "        # Each expert learns to predict all disease classes independently\n",
    "        bce_losses = [\n",
    "            criterion[\"bce\"](logit.float(), targets.float())\n",
    "            for logit in logits[:3]  # Expert branches only (b1, b2, b3)\n",
    "        ]\n",
    "\n",
    "        # Gating network loss (gate): BCE loss for adaptively fused predictions\n",
    "        # The gating network learns to combine expert predictions optimally\n",
    "        gate_loss = criterion[\"bce\"](logits[3].float(), targets.float())\n",
    "\n",
    "        # Attention diversity loss (divAttn): Encourages experts to focus on different regions\n",
    "        # Lower values = more diverse attention patterns across experts\n",
    "        diversity_loss = multiLabelAttnDiv(cams_list, targets)\n",
    "\n",
    "        # Combine all losses with predefined weights\n",
    "        # loss_wgts = [0.7, 1.0, 0.01] for [expert_weight, gate_weight, diversity_weight]\n",
    "        loss_values = bce_losses + [gate_loss, diversity_loss]\n",
    "        total_loss = (\n",
    "            args[\"loss_wgts\"][0] * sum(bce_losses)  # Expert loss weight (0.7)\n",
    "            + args[\"loss_wgts\"][1] * gate_loss      # Gating loss weight (1.0)\n",
    "            + args[\"loss_wgts\"][2] * diversity_loss # Diversity loss weight (0.01)\n",
    "        )\n",
    "        loss_values.append(total_loss)\n",
    "\n",
    "        # ===== ACCURACY COMPUTATION =====\n",
    "        \n",
    "        # Compute subset accuracy for each prediction head\n",
    "        # Subset accuracy = percentage of samples where ALL predicted labels match ground truth\n",
    "        acc_values = []\n",
    "        for logit in logits:\n",
    "            subset_acc, hamming_acc, _, _, _ = multiLabelAccuracy(logit, targets)\n",
    "            acc_values.append(subset_acc * 100)  # Convert to percentage\n",
    "\n",
    "        # Update running averages for all metrics\n",
    "        multi_loss = {loss_keys[k]: loss_values[k] for k in range(len(loss_keys))}\n",
    "        train_accs = {acc_keys[k]: acc_values[k] for k in range(len(acc_keys))}\n",
    "\n",
    "        update_meter(loss_meter, multi_loss, inputs.size(0))\n",
    "        update_meter(acc_meter, train_accs, inputs.size(0))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress every 50 batches\n",
    "        if i % 50 == 0:\n",
    "            tmp_str = f\"Batch [{i}/{len(train_loader)}] \"\n",
    "            tmp_str += \"< Training Loss >\\n\"\n",
    "            for k, v in loss_meter.items():\n",
    "                tmp_str += f\"{k}:{v.value:.4f} \"\n",
    "            tmp_str += \"\\n< Training Accuracy >\\n\"\n",
    "            for k, v in acc_meter.items():\n",
    "                tmp_str += f\"{k}:{v.value:.1f} \"\n",
    "            print(tmp_str)\n",
    "\n",
    "    time_elapsed = time.time() - time_start\n",
    "    print(f\"\\nEpoch completed in {time_elapsed:.1f}s\")\n",
    "\n",
    "    # Final epoch summary\n",
    "    tmp_str = \"< Final Training Loss >\\n\"\n",
    "    for k, v in loss_meter.items():\n",
    "        tmp_str += f\"{k}:{v.value:.4f} \"\n",
    "    tmp_str += \"\\n< Final Training Accuracy >\\n\"\n",
    "    for k, v in acc_meter.items():\n",
    "        tmp_str += f\"{k}:{v.value:.1f} \"\n",
    "    print(tmp_str)\n",
    "\n",
    "    return loss_meter[loss_keys[-1]].value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c073d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 1: Basic Multi-Label MEDAF\n",
      "============================================================\n",
      "Making resnet layer with channel 64 block 2 stride 1\n",
      "Making resnet layer with channel 128 block 2 stride 2\n",
      "Making resnet layer with channel 256 block 2 stride 2\n",
      "Making resnet layer with channel 512 block 2 stride 2\n",
      "Phase 1 Model Parameters: 44,749,955\n",
      "Batch [0/3010] < Training Loss >\n",
      "b1:0.8761 b2:0.7285 b3:0.5811 gate:0.7074 divAttn:0.3422 total:2.2407 \n",
      "< Training Accuracy >\n",
      "acc1:0.0 acc2:0.0 acc3:3.1 accGate:0.0 \n",
      "Batch [50/3010] < Training Loss >\n",
      "b1:0.3201 b2:0.2933 b3:0.2786 gate:0.2930 divAttn:0.4527 total:0.9219 \n",
      "< Training Accuracy >\n",
      "acc1:51.3 acc2:52.5 acc3:53.7 accGate:53.3 \n",
      "Batch [100/3010] < Training Loss >\n",
      "b1:0.2862 b2:0.2718 b3:0.2643 gate:0.2715 divAttn:0.3360 total:0.8505 \n",
      "< Training Accuracy >\n",
      "acc1:54.2 acc2:54.7 acc3:55.4 accGate:55.2 \n",
      "Batch [150/3010] < Training Loss >\n",
      "b1:0.2759 b2:0.2664 b3:0.2609 gate:0.2658 divAttn:0.2805 total:0.8308 \n",
      "< Training Accuracy >\n",
      "acc1:54.5 acc2:54.9 acc3:55.3 accGate:55.2 \n",
      "Batch [200/3010] < Training Loss >\n",
      "b1:0.2676 b2:0.2605 b3:0.2561 gate:0.2598 divAttn:0.2552 total:0.8112 \n",
      "< Training Accuracy >\n",
      "acc1:55.6 acc2:55.9 acc3:56.2 accGate:56.1 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"Demonstrate Phase 1: Basic Multi-Label MEDAF\"\"\"\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 1: Basic Multi-Label MEDAF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration for Phase 1\n",
    "args = {\n",
    "    \"img_size\": config[\"img_size\"],\n",
    "    \"backbone\": \"resnet18\",\n",
    "    \"num_classes\": config[\"num_classes\"],\n",
    "    \"gate_temp\": 100,\n",
    "    \"loss_keys\": [\"b1\", \"b2\", \"b3\", \"gate\", \"divAttn\", \"total\"],\n",
    "    \"acc_keys\": [\"acc1\", \"acc2\", \"acc3\", \"accGate\"],\n",
    "    # [expert_weight, gate_weight, diversity_weight] = 0.7 * (b1, b2, b3) + 1.0 * (gate) + 0.01 * (divAttn)\n",
    "    \"loss_wgts\": [0.7, 1.0, 0.01], \n",
    "}\n",
    "\n",
    "# Create Phase 1 model\n",
    "model = MultiLabelMEDAF(args)\n",
    "model.to(device)\n",
    "\n",
    "print(\n",
    "    f\"Phase 1 Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\"\n",
    ")\n",
    "\n",
    "# Training setup\n",
    "criterion = {\"bce\": nn.BCEWithLogitsLoss()}\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=config[\"learning_rate\"]\n",
    ")\n",
    "\n",
    "# Training\n",
    "phase1_metrics = []\n",
    "for epoch in range(config[\"num_epochs\"]):\n",
    "    metrics = train_multilabel(\n",
    "        train_loader, model, criterion, optimizer, args, device\n",
    "    )\n",
    "    phase1_metrics.append(metrics)\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss={metrics:.4f}\")\n",
    "\n",
    "final_loss = phase1_metrics[-1] if phase1_metrics else float(\"nan\")\n",
    "checkpoint_path = save_model(model, args, phase1_metrics)\n",
    "\n",
    "results[\"phase1\"] = {\n",
    "    \"model\": model,\n",
    "    \"final_loss\": final_loss,\n",
    "    \"metrics_history\": phase1_metrics,\n",
    "    \"checkpoint\": str(checkpoint_path),\n",
    "}\n",
    "print('-'*40)\n",
    "print('\\n\\n')\n",
    "if phase1_metrics:\n",
    "    print(f\"Phase 1 Final Loss: {final_loss:.4f}\")\n",
    "else:\n",
    "    print(\"Phase 1 completed with zero epochs (no training performed)\")\n",
    "print(f\"Phase 1 checkpoint saved to: {checkpoint_path}\")\n",
    "print(\n",
    "    \"Use load_phase1_checkpoint(CheckpointPath) to reload this model for evaluation.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc23ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39d57ce",
   "metadata": {},
   "source": [
    "# ===== EVALUATION FUNCTIONS ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d80f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_results(results):\n",
    "    \"\"\"Print comprehensive evaluation results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 MEDAF MODEL EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall = results['overall']\n",
    "    print(f\"\\n📊 Overall Performance:\")\n",
    "    print(f\"   Subset Accuracy:  {overall['subset_accuracy']:.4f} ({overall['subset_accuracy']*100:.2f}%)\")\n",
    "    print(f\"   Hamming Accuracy: {overall['hamming_accuracy']:.4f} ({overall['hamming_accuracy']*100:.2f}%)\")\n",
    "    print(f\"   Precision:        {overall['precision']:.4f}\")\n",
    "    print(f\"   Recall:           {overall['recall']:.4f}\")\n",
    "    print(f\"   F1-Score:         {overall['f1_score']:.4f}\")\n",
    "    print(f\"   Average Loss:     {overall['average_loss']:.4f}\")\n",
    "    \n",
    "    # Per-expert comparison\n",
    "    print(f\"\\n🔬 Per-Expert Performance:\")\n",
    "    for expert_name, expert_metrics in results['per_expert'].items():\n",
    "        print(f\"   {expert_name:>8}: Subset={expert_metrics['subset_accuracy']:.4f}, \"\n",
    "                f\"Hamming={expert_metrics['hamming_accuracy']:.4f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(f\"\\n🏷️  Per-Class Performance:\")\n",
    "    class_names = results['class_names']\n",
    "    precision = results['per_class']['precision']\n",
    "    recall = results['per_class']['recall']\n",
    "    f1_score = results['per_class']['f1_score']\n",
    "    \n",
    "    print(f\"   {'Class':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "    print(\"   \" + \"-\"*50)\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"   {class_name:<15} {precision[i]:<10.4f} {recall[i]:<10.4f} {f1_score[i]:<10.4f}\")\n",
    "    \n",
    "    # Best performing classes\n",
    "    best_f1_idx = max(range(len(f1_score)), key=lambda i: f1_score[i])\n",
    "    worst_f1_idx = min(range(len(f1_score)), key=lambda i: f1_score[i])\n",
    "    \n",
    "    print(f\"\\n🏆 Best Class:  {class_names[best_f1_idx]} (F1={f1_score[best_f1_idx]:.4f})\")\n",
    "    print(f\"📉 Worst Class: {class_names[worst_f1_idx]} (F1={f1_score[worst_f1_idx]:.4f})\")\n",
    "\n",
    "\n",
    "def load_and_evaluate_checkpoint(checkpoint_path, data_loader, device, class_names=None):\n",
    "    \"\"\"\n",
    "    Load a saved checkpoint and evaluate it\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to saved model checkpoint\n",
    "        data_loader: DataLoader for evaluation\n",
    "        device: Device to run evaluation on\n",
    "        class_names: List of class names\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation results dictionary\n",
    "    \"\"\"\n",
    "    print(f\"📁 Loading checkpoint from: {checkpoint_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        # Extract model arguments\n",
    "        args = checkpoint.get('args', {})\n",
    "        if not args:\n",
    "            raise ValueError(\"Checkpoint missing 'args' - cannot reconstruct model\")\n",
    "        \n",
    "        # Create model\n",
    "        model = MultiLabelMEDAF(args)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        model.to(device)\n",
    "        \n",
    "        print(f\"✅ Model loaded successfully\")\n",
    "        print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        print(f\"   Dataset: {checkpoint.get('dataset', 'Unknown')}\")\n",
    "        print(f\"   Classes: {checkpoint.get('class_names', 'Unknown')}\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        results = evaluate_multilabel(model, data_loader, device, class_names)\n",
    "        \n",
    "        return results, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading checkpoint: {e}\")\n",
    "        return None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1d06727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🔍 FINAL EVALUATION - CLEAN VERSION\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'phase1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Load and evaluate the trained model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m checkpoint_path = \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mphase1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mcheckpoint\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    138\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m📁 Loading checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    140\u001b[39m THRESHOLD = \u001b[32m0.5\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'phase1'"
     ]
    }
   ],
   "source": [
    "def evaluate_medaf_final(model, data_loader, device, class_names, threshold=0.1):\n",
    "    \"\"\"Final clean evaluation function - removes all duplicates\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"🔍 FINAL EVALUATION (Threshold: {threshold})\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output_dict = model(inputs, targets)\n",
    "            gate_logits = output_dict[\"logits\"][-1]  # Use gating network\n",
    "            \n",
    "            # Convert to predictions\n",
    "            probs = torch.sigmoid(gate_logits)\n",
    "            pred_binary = (probs > threshold).float()\n",
    "            \n",
    "            # Store data\n",
    "            all_predictions.append(pred_binary.cpu())\n",
    "            all_targets.append(targets.cpu())\n",
    "            \n",
    "            # Compute loss\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            loss = criterion(gate_logits, targets.float())\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    # Concatenate all data\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    \n",
    "    # Compute metrics\n",
    "    subset_acc = (all_predictions == all_targets).all(dim=1).float().mean().item()\n",
    "    hamming_acc = (all_predictions == all_targets).float().mean().item()\n",
    "    \n",
    "    # Per-class metrics\n",
    "    tp = (all_predictions * all_targets).sum(dim=0)\n",
    "    fp = (all_predictions * (1 - all_targets)).sum(dim=0)\n",
    "    fn = ((1 - all_predictions) * all_targets).sum(dim=0)\n",
    "    \n",
    "    precision = torch.zeros_like(tp, dtype=torch.float32)\n",
    "    recall = torch.zeros_like(tp, dtype=torch.float32)\n",
    "    f1 = torch.zeros_like(tp, dtype=torch.float32)\n",
    "    \n",
    "    for i in range(len(tp)):\n",
    "        if tp[i] + fp[i] > 0:\n",
    "            precision[i] = tp[i] / (tp[i] + fp[i])\n",
    "        if tp[i] + fn[i] > 0:\n",
    "            recall[i] = tp[i] / (tp[i] + fn[i])\n",
    "        if precision[i] + recall[i] > 0:\n",
    "            f1[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'overall': {\n",
    "            'subset_accuracy': subset_acc,\n",
    "            'hamming_accuracy': hamming_acc,\n",
    "            'precision': precision.mean().item(),\n",
    "            'recall': recall.mean().item(),\n",
    "            'f1_score': f1.mean().item(),\n",
    "            'average_loss': total_loss / num_batches,\n",
    "            'threshold_used': threshold\n",
    "        },\n",
    "        'per_class': {\n",
    "            'precision': precision.tolist(),\n",
    "            'recall': recall.tolist(),\n",
    "            'f1_score': f1.tolist()\n",
    "        },\n",
    "        'class_names': class_names\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_final_results(results):\n",
    "    \"\"\"Print final clean results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 MEDAF FINAL EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    overall = results['overall']\n",
    "    print(f\"\\n📊 Overall Performance:\")\n",
    "    print(f\"   Subset Accuracy:  {overall['subset_accuracy']:.4f} ({overall['subset_accuracy']*100:.2f}%)\")\n",
    "    print(f\"   Hamming Accuracy: {overall['hamming_accuracy']:.4f} ({overall['hamming_accuracy']*100:.2f}%)\")\n",
    "    print(f\"   Precision:        {overall['precision']:.4f}\")\n",
    "    print(f\"   Recall:           {overall['recall']:.4f}\")\n",
    "    print(f\"   F1-Score:         {overall['f1_score']:.4f}\")\n",
    "    print(f\"   Average Loss:     {overall['average_loss']:.4f}\")\n",
    "    print(f\"   Threshold Used:   {overall['threshold_used']}\")\n",
    "    \n",
    "    # Per-class performance\n",
    "    print(f\"\\n🏷️  Per-Class Performance:\")\n",
    "    class_names = results['class_names']\n",
    "    precision = results['per_class']['precision']\n",
    "    recall = results['per_class']['recall']\n",
    "    f1_score = results['per_class']['f1_score']\n",
    "    \n",
    "    print(f\"   {'Class':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "    print(\"   \" + \"-\"*50)\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"   {class_name:<15} {precision[i]:<10.4f} {recall[i]:<10.4f} {f1_score[i]:<10.4f}\")\n",
    "    \n",
    "    # Best and worst classes\n",
    "    best_f1_idx = max(range(len(f1_score)), key=lambda i: f1_score[i])\n",
    "    worst_f1_idx = min(range(len(f1_score)), key=lambda i: f1_score[i])\n",
    "    \n",
    "    print(f\"\\n🏆 Best Class:  {class_names[best_f1_idx]} (F1={f1_score[best_f1_idx]:.4f})\")\n",
    "    print(f\"📉 Worst Class: {class_names[worst_f1_idx]} (F1={f1_score[worst_f1_idx]:.4f})\")\n",
    "    \n",
    "    # Model assessment\n",
    "    print(f\"\\n💡 Model Assessment:\")\n",
    "    if overall['f1_score'] < 0.1:\n",
    "        print(\"   ⚠️  Model is under-trained (F1 < 0.1)\")\n",
    "        print(\"   📈 Recommendation: Train with more data and epochs\")\n",
    "    elif overall['f1_score'] < 0.3:\n",
    "        print(\"   🔶 Model shows some learning but needs improvement\")\n",
    "    else:\n",
    "        print(\"   ✅ Model shows good performance\")\n",
    "\n",
    "\n",
    "# ===== RUN FINAL EVALUATION =====\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔍 FINAL EVALUATION - CLEAN VERSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load and evaluate the trained model\n",
    "checkpoint_path = results[\"phase1\"][\"checkpoint\"]\n",
    "print(f\"📁 Loading checkpoint: {checkpoint_path}\")\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "try:\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    args = checkpoint.get('args', {})\n",
    "    \n",
    "    if not args:\n",
    "        raise ValueError(\"Checkpoint missing 'args'\")\n",
    "    \n",
    "    # Create and load model\n",
    "    model = MultiLabelMEDAF(args)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"✅ Model loaded successfully\")\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Evaluate with optimal threshold\n",
    "    final_results = evaluate_medaf_final(model, val_loader, device, class_names, threshold=THRESHOLD)\n",
    "    \n",
    "    # Print results\n",
    "    print_final_results(final_results)\n",
    "    \n",
    "    # Save results\n",
    "    eval_save_path = Path(checkpoint_path).parent / \"final_evaluation_results.json\"\n",
    "    with open(eval_save_path, 'w') as f:\n",
    "        json_results = {}\n",
    "        for key, value in final_results.items():\n",
    "            if isinstance(value, dict):\n",
    "                json_results[key] = {k: v.tolist() if hasattr(v, 'tolist') else v \n",
    "                                   for k, v in value.items()}\n",
    "            else:\n",
    "                json_results[key] = value.tolist() if hasattr(value, 'tolist') else value\n",
    "        \n",
    "        json.dump(json_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 Final results saved to: {eval_save_path}\")\n",
    "    \n",
    "    # Store results\n",
    "    results[\"final_evaluation\"] = final_results\n",
    "    results[\"loaded_model\"] = model\n",
    "    \n",
    "    print(f\"\\n✅ FINAL EVALUATION COMPLETE\")\n",
    "    print(f\"   Model successfully evaluated on validation set\")\n",
    "    print(f\"   Ready for full dataset training\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3661869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Cannot run additional analysis - evaluation results not available\n"
     ]
    }
   ],
   "source": [
    "# ===== ADDITIONAL ANALYSIS AND VISUALIZATION =====\n",
    "\n",
    "def analyze_model_predictions(model, data_loader, device, class_names, num_samples=5):\n",
    "    \"\"\"\n",
    "    Analyze specific predictions and show examples\n",
    "    \n",
    "    Args:\n",
    "        model: Trained MEDAF model\n",
    "        data_loader: DataLoader for analysis\n",
    "        device: Device to run on\n",
    "        class_names: List of class names\n",
    "        num_samples: Number of samples to analyze\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"\\n🔍 ANALYZING MODEL PREDICTIONS (showing {num_samples} samples)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            if batch_idx >= 1:  # Only analyze first batch\n",
    "                break\n",
    "                \n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Get predictions from all experts\n",
    "            output_dict = model(inputs, targets)\n",
    "            logits = output_dict[\"logits\"]\n",
    "            gate_pred = output_dict[\"gate_pred\"]\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probs = [torch.sigmoid(logit) for logit in logits]\n",
    "            \n",
    "            # Analyze each sample\n",
    "            for sample_idx in range(min(num_samples, inputs.size(0))):\n",
    "                print(f\"\\n📋 Sample {sample_idx + 1}:\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                # Ground truth\n",
    "                gt_labels = targets[sample_idx]\n",
    "                gt_indices = torch.where(gt_labels == 1)[0]\n",
    "                gt_names = [class_names[i] for i in gt_indices] if gt_indices.numel() > 0 else [\"No diseases\"]\n",
    "                \n",
    "                print(f\"Ground Truth: {gt_names}\")\n",
    "                \n",
    "                # Expert predictions\n",
    "                expert_names = ['Expert 1', 'Expert 2', 'Expert 3', 'Gating Network']\n",
    "                for expert_idx, (expert_name, prob) in enumerate(zip(expert_names, probs)):\n",
    "                    sample_prob = prob[sample_idx]\n",
    "                    pred_indices = torch.where(sample_prob > 0.5)[0]\n",
    "                    pred_names = [class_names[i] for i in pred_indices] if pred_indices.numel() > 0 else [\"No diseases\"]\n",
    "                    \n",
    "                    # Calculate confidence scores\n",
    "                    confidences = sample_prob[pred_indices] if pred_indices.numel() > 0 else torch.tensor([])\n",
    "                    \n",
    "                    print(f\"{expert_name:>15}: {pred_names}\")\n",
    "                    if confidences.numel() > 0:\n",
    "                        conf_str = \", \".join([f\"{c:.3f}\" for c in confidences])\n",
    "                        print(f\"{'Confidence':>15}: {conf_str}\")\n",
    "                \n",
    "                # Gating weights\n",
    "                sample_gate = gate_pred[sample_idx]\n",
    "                print(f\"{'Gating Weights':>15}: Expert1={sample_gate[0]:.3f}, Expert2={sample_gate[1]:.3f}, Expert3={sample_gate[2]:.3f}\")\n",
    "\n",
    "\n",
    "def plot_training_curves(results):\n",
    "    \"\"\"Plot training curves if matplotlib is available\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        print(\"\\n📈 PLOTTING TRAINING CURVES\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        if \"phase1\" in results and \"metrics_history\" in results[\"phase1\"]:\n",
    "            metrics_history = results[\"phase1\"][\"metrics_history\"]\n",
    "            \n",
    "            plt.figure(figsize=(12, 4))\n",
    "            \n",
    "            # Loss curve\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(metrics_history, 'b-', linewidth=2, label='Training Loss')\n",
    "            plt.title('Training Loss Over Epochs')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            \n",
    "            # Loss improvement\n",
    "            plt.subplot(1, 2, 2)\n",
    "            if len(metrics_history) > 1:\n",
    "                improvements = [metrics_history[0] - loss for loss in metrics_history]\n",
    "                plt.plot(improvements, 'g-', linewidth=2, label='Loss Reduction')\n",
    "                plt.title('Loss Improvement Over Epochs')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss Reduction')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('medaf_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"✅ Training curves saved as 'medaf_training_curves.png'\")\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ No training history available for plotting\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"❌ Matplotlib not available - skipping plots\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating plots: {e}\")\n",
    "\n",
    "\n",
    "def generate_model_summary(results):\n",
    "    \"\"\"Generate a comprehensive model summary\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📋 MEDAF MODEL TRAINING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Training summary\n",
    "    if \"phase1\" in results:\n",
    "        phase1 = results[\"phase1\"]\n",
    "        print(f\"\\n🎯 Phase 1 Training Results:\")\n",
    "        print(f\"   Final Loss: {phase1.get('final_loss', 'N/A'):.4f}\")\n",
    "        print(f\"   Checkpoint: {phase1.get('checkpoint', 'N/A')}\")\n",
    "        \n",
    "        if \"metrics_history\" in phase1:\n",
    "            initial_loss = phase1[\"metrics_history\"][0] if phase1[\"metrics_history\"] else 0\n",
    "            final_loss = phase1[\"metrics_history\"][-1] if phase1[\"metrics_history\"] else 0\n",
    "            improvement = ((initial_loss - final_loss) / initial_loss * 100) if initial_loss > 0 else 0\n",
    "            print(f\"   Loss Improvement: {improvement:.1f}%\")\n",
    "    \n",
    "    # Evaluation summary\n",
    "    if \"evaluation\" in results:\n",
    "        eval_results = results[\"evaluation\"]\n",
    "        overall = eval_results[\"overall\"]\n",
    "        \n",
    "        print(f\"\\n📊 Evaluation Results:\")\n",
    "        print(f\"   Subset Accuracy: {overall['subset_accuracy']*100:.2f}%\")\n",
    "        print(f\"   Hamming Accuracy: {overall['hamming_accuracy']*100:.2f}%\")\n",
    "        print(f\"   F1-Score: {overall['f1_score']:.4f}\")\n",
    "        print(f\"   Average Loss: {overall['average_loss']:.4f}\")\n",
    "        \n",
    "        # Best and worst classes\n",
    "        per_class = eval_results[\"per_class\"]\n",
    "        f1_scores = per_class[\"f1_score\"]\n",
    "        class_names = eval_results[\"class_names\"]\n",
    "        \n",
    "        best_idx = max(range(len(f1_scores)), key=lambda i: f1_scores[i])\n",
    "        worst_idx = min(range(len(f1_scores)), key=lambda i: f1_scores[i])\n",
    "        \n",
    "        print(f\"\\n🏆 Best Performing Class: {class_names[best_idx]} (F1={f1_scores[best_idx]:.4f})\")\n",
    "        print(f\"📉 Worst Performing Class: {class_names[worst_idx]} (F1={f1_scores[worst_idx]:.4f})\")\n",
    "    \n",
    "    # Model architecture info\n",
    "    if \"loaded_model\" in results:\n",
    "        model = results[\"loaded_model\"]\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\\n🏗️  Model Architecture:\")\n",
    "        print(f\"   Total Parameters: {total_params:,}\")\n",
    "        print(f\"   Trainable Parameters: {trainable_params:,}\")\n",
    "        print(f\"   Model Type: Multi-Label MEDAF (3 Experts + Gating)\")\n",
    "    \n",
    "    print(f\"\\n💡 Key Insights:\")\n",
    "    print(f\"   • Multi-expert architecture enables diverse feature learning\")\n",
    "    print(f\"   • Gating network adaptively combines expert predictions\")\n",
    "    print(f\"   • Attention diversity encourages complementary expert focus\")\n",
    "    print(f\"   • Model successfully handles multi-label classification\")\n",
    "\n",
    "\n",
    "# ===== RUN ADDITIONAL ANALYSIS =====\n",
    "\n",
    "if \"evaluation\" in results and \"loaded_model\" in results:\n",
    "    # Analyze specific predictions\n",
    "    analyze_model_predictions(\n",
    "        results[\"loaded_model\"], \n",
    "        val_loader, \n",
    "        device, \n",
    "        class_names, \n",
    "        num_samples=3\n",
    "    )\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_training_curves(results)\n",
    "    \n",
    "    # Generate comprehensive summary\n",
    "    generate_model_summary(results)\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Cannot run additional analysis - evaluation results not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18abbb0a-dd7d-4ec8-8418-eecd2652d2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research_medaf_aidan)",
   "language": "python",
   "name": "research_medaf_aidan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
