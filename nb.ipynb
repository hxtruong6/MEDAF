{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a065071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "#!/usr/bin/env python3\n",
    "import ast\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import Phase 1 (basic multi-label)\n",
    "from core.multilabel_net import MultiLabelMEDAF\n",
    "from core.multilabel_train import train_multilabel\n",
    "\n",
    "# Import Phase 2 (per-class gating)\n",
    "from core.multilabel_net_v2 import MultiLabelMEDAFv2\n",
    "from core.multilabel_train_v2 import train_multilabel_v2, ComparativeTrainingFramework\n",
    "\n",
    "# Import test utilities (synthetic dataset fallback)\n",
    "from test_multilabel_medaf import SyntheticMultiLabelDataset\n",
    "\n",
    "torch.__version__, torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ef557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Paths\n",
    "KNOWN_LABELS = [\n",
    "    \"Atelectasis\",\n",
    "    \"Cardiomegaly\",\n",
    "    \"Effusion\",\n",
    "    \"Infiltration\",\n",
    "    \"Mass\",\n",
    "    \"Nodule\",\n",
    "    \"Pneumonia\",\n",
    "    \"Pneumothorax\",\n",
    "]\n",
    "\n",
    "DEFAULT_IMAGE_ROOT = Path(\"datasets/data/NIH/images-224\")\n",
    "DEFAULT_KNOWN_CSV = Path(\"datasets/data/NIH/chestxray_train_known.csv\")\n",
    "DEFAULT_CHECKPOINT_DIR = Path(\"checkpoints/medaf_phase1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: ChestXrayKnownDataset\n",
    "class ChestXrayKnownDataset(data.Dataset):\n",
    "    \"\"\"Dataset that reads the known-label ChestX-ray14 split for Phase 1 training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: Path,\n",
    "        image_root: Path,\n",
    "        img_size: int = 224,\n",
    "        max_samples: Optional[int] = None,\n",
    "        transform=None,\n",
    "    ) -> None:\n",
    "        self.csv_path = Path(csv_path)\n",
    "        self.image_root = Path(image_root)\n",
    "        self.img_size = img_size\n",
    "        self.class_names = KNOWN_LABELS\n",
    "        self.num_classes = len(self.class_names)\n",
    "\n",
    "        if not self.csv_path.exists():\n",
    "            raise FileNotFoundError(f\"ChestX-ray CSV not found: {self.csv_path}\")\n",
    "        if not self.image_root.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"ChestX-ray image directory not found: {self.image_root}\"\n",
    "            )\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((img_size, img_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225],\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        if \"known_labels\" not in df.columns:\n",
    "            raise ValueError(\n",
    "                \"Expected 'known_labels' column in CSV. Run utils/create_chestxray_splits.py first.\"\n",
    "            )\n",
    "\n",
    "        if max_samples is not None and max_samples < len(df):\n",
    "            df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "        self.records = df.to_dict(\"records\")\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.class_names)}\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_label_list(raw_value):\n",
    "        if isinstance(raw_value, list):\n",
    "            return raw_value\n",
    "        if pd.isna(raw_value):\n",
    "            return []\n",
    "        if isinstance(raw_value, str):\n",
    "            raw_value = raw_value.strip()\n",
    "            if not raw_value:\n",
    "                return []\n",
    "            try:\n",
    "                parsed = ast.literal_eval(raw_value)\n",
    "                if isinstance(parsed, (list, tuple)):\n",
    "                    return list(parsed)\n",
    "                if isinstance(parsed, str):\n",
    "                    return [parsed]\n",
    "            except (ValueError, SyntaxError):\n",
    "                pass\n",
    "            return [item.strip() for item in raw_value.split(\"|\") if item.strip()]\n",
    "        return []\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.records[idx]\n",
    "        image_path = self.image_root / record[\"Image Index\"]\n",
    "        if not image_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing image: {image_path}\")\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        labels = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        for label in self._parse_label_list(record.get(\"known_labels\", [])):\n",
    "            if label in self.label_to_idx:\n",
    "                labels[self.label_to_idx[label]] = 1.0\n",
    "\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo Class: MultiLabelMEDAFDemo\n",
    "class MultiLabelMEDAFDemo:\n",
    "    \"\"\"\n",
    "    Comprehensive demo for Multi-Label MEDAF\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.config.setdefault(\"data_source\", \"chestxray\")\n",
    "        self.config.setdefault(\"img_size\", 224)\n",
    "        self.config.setdefault(\"num_classes\", len(KNOWN_LABELS))\n",
    "        self.config.setdefault(\"val_ratio\", 0.1)\n",
    "        self.config.setdefault(\"num_workers\", 0)\n",
    "        self.config.setdefault(\"checkpoint_dir\", str(DEFAULT_CHECKPOINT_DIR))\n",
    "        self.config.setdefault(\"phase1_checkpoint\", \"medaf_phase1_chestxray.pt\")\n",
    "        self.config.setdefault(\"num_samples\", 1000)\n",
    "        self.config.setdefault(\"avg_labels_per_sample\", 3)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.results = {}\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.test_loader = None\n",
    "        self.dataset_name = None\n",
    "        self.class_names = KNOWN_LABELS\n",
    "\n",
    "    def create_dataset(self):\n",
    "        \"\"\"Create train/validation loaders for Phase 1.\"\"\"\n",
    "\n",
    "        data_source = self.config.get(\"data_source\", \"chestxray\").lower()\n",
    "\n",
    "        if data_source == \"chestxray\":\n",
    "            csv_path = Path(self.config.get(\"known_csv\", DEFAULT_KNOWN_CSV))\n",
    "            image_root = Path(self.config.get(\"image_root\", DEFAULT_IMAGE_ROOT))\n",
    "            max_samples = self.config.get(\"max_samples\")\n",
    "            if isinstance(max_samples, str):\n",
    "                max_samples = int(max_samples)\n",
    "            print(f\"Loading ChestX-ray14 known-label split from {csv_path}\")\n",
    "            dataset = ChestXrayKnownDataset(\n",
    "                csv_path=csv_path,\n",
    "                image_root=image_root,\n",
    "                img_size=self.config.get(\"img_size\", 224),\n",
    "                max_samples=max_samples,\n",
    "            )\n",
    "            self.dataset_name = \"ChestX-ray14 (known labels)\"\n",
    "            self.class_names = dataset.class_names\n",
    "            self.config[\"num_classes\"] = dataset.num_classes\n",
    "            self.config[\"img_size\"] = dataset.img_size\n",
    "        else:\n",
    "            print(\n",
    "                \"ChestX-ray data not requested or unavailable. Falling back to synthetic dataset.\"\n",
    "            )\n",
    "            dataset = SyntheticMultiLabelDataset(\n",
    "                num_samples=self.config.get(\"num_samples\", 1000),\n",
    "                img_size=self.config.get(\"img_size\", 32),\n",
    "                num_classes=self.config.get(\"num_classes\", 8),\n",
    "                avg_labels_per_sample=self.config.get(\"avg_labels_per_sample\", 3),\n",
    "                random_state=42,\n",
    "            )\n",
    "            self.dataset_name = \"Synthetic\"\n",
    "            self.class_names = [f\"class_{i}\" for i in range(dataset.num_classes)]\n",
    "\n",
    "        val_ratio = float(self.config.get(\"val_ratio\", 0.1))\n",
    "        val_size = max(1, int(len(dataset) * val_ratio))\n",
    "        train_size = len(dataset) - val_size\n",
    "\n",
    "        train_dataset, val_dataset = data.random_split(\n",
    "            dataset,\n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(42),\n",
    "        )\n",
    "\n",
    "        batch_size = self.config.get(\"batch_size\", 16)\n",
    "        num_workers = self.config.get(\"num_workers\", 4)\n",
    "        pin_memory = torch.cuda.is_available()\n",
    "\n",
    "        self.train_loader = data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "        )\n",
    "\n",
    "        self.val_loader = data.DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "        )\n",
    "\n",
    "        self.test_loader = self.val_loader\n",
    "\n",
    "        print(\n",
    "            f\"Dataset prepared: {train_size} train / {val_size} val samples ({self.dataset_name})\"\n",
    "        )\n",
    "\n",
    "    def demo_phase1(self):\n",
    "        \"\"\"Demonstrate Phase 1: Basic Multi-Label MEDAF\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"PHASE 1 DEMO: Basic Multi-Label MEDAF\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Configuration for Phase 1\n",
    "        args = {\n",
    "            \"img_size\": self.config[\"img_size\"],\n",
    "            \"backbone\": \"resnet18\",\n",
    "            \"num_classes\": self.config[\"num_classes\"],\n",
    "            \"gate_temp\": 100,\n",
    "            \"loss_keys\": [\"b1\", \"b2\", \"b3\", \"gate\", \"divAttn\", \"total\"],\n",
    "            \"acc_keys\": [\"acc1\", \"acc2\", \"acc3\", \"accGate\"],\n",
    "            \"loss_wgts\": [0.7, 1.0, 0.01],\n",
    "        }\n",
    "\n",
    "        # Create Phase 1 model\n",
    "        model = MultiLabelMEDAF(args)\n",
    "        model.to(self.device)\n",
    "\n",
    "        print(\n",
    "            f\"Phase 1 Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\"\n",
    "        )\n",
    "\n",
    "        # Training setup\n",
    "        criterion = {\"bce\": nn.BCEWithLogitsLoss()}\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=self.config[\"learning_rate\"]\n",
    "        )\n",
    "\n",
    "        # Training\n",
    "        phase1_metrics = []\n",
    "        for epoch in range(self.config[\"num_epochs\"]):\n",
    "            metrics = train_multilabel(\n",
    "                self.train_loader, model, criterion, optimizer, args, self.device\n",
    "            )\n",
    "            phase1_metrics.append(metrics)\n",
    "\n",
    "            if epoch % 2 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss={metrics:.4f}\")\n",
    "\n",
    "        final_loss = phase1_metrics[-1] if phase1_metrics else float(\"nan\")\n",
    "        checkpoint_path = self.save_model(model, args, phase1_metrics)\n",
    "\n",
    "        self.results[\"phase1\"] = {\n",
    "            \"model\": model,\n",
    "            \"final_loss\": final_loss,\n",
    "            \"metrics_history\": phase1_metrics,\n",
    "            \"checkpoint\": str(checkpoint_path),\n",
    "        }\n",
    "\n",
    "        if phase1_metrics:\n",
    "            print(f\"Phase 1 Final Loss: {final_loss:.4f}\")\n",
    "        else:\n",
    "            print(\"Phase 1 completed with zero epochs (no training performed)\")\n",
    "        print(f\"Phase 1 checkpoint saved to: {checkpoint_path}\")\n",
    "        print(\n",
    "            \"Use load_phase1_checkpoint(CheckpointPath) to reload this model for evaluation.\"\n",
    "        )\n",
    "\n",
    "    def save_model(self, model, args, loss_history):\n",
    "        ckpt_dir = Path(self.config[\"checkpoint_dir\"])\n",
    "        ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        checkpoint_path = ckpt_dir / self.config[\"phase1_checkpoint\"]\n",
    "\n",
    "        payload = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"args\": args,\n",
    "            \"class_names\": self.class_names,\n",
    "            \"dataset\": self.dataset_name,\n",
    "        }\n",
    "        torch.save(payload, checkpoint_path)\n",
    "\n",
    "        metadata = {\n",
    "            \"dataset\": self.dataset_name,\n",
    "            \"class_names\": self.class_names,\n",
    "            \"num_epochs\": self.config[\"num_epochs\"],\n",
    "            \"batch_size\": self.config.get(\"batch_size\"),\n",
    "            \"learning_rate\": self.config.get(\"learning_rate\"),\n",
    "            \"loss_history\": [float(loss) for loss in loss_history],\n",
    "            \"device\": str(self.device),\n",
    "            \"checkpoint\": str(checkpoint_path),\n",
    "            \"config\": {\n",
    "                k: v\n",
    "                for k, v in self.config.items()\n",
    "                if isinstance(v, (int, float, str, bool))\n",
    "            },\n",
    "        }\n",
    "        metadata_path = checkpoint_path.with_suffix(\".json\")\n",
    "        with metadata_path.open(\"w\", encoding=\"utf-8\") as fp:\n",
    "            json.dump(metadata, fp, indent=2)\n",
    "\n",
    "        return checkpoint_path\n",
    "\n",
    "    def demo_phase2_comparative(self):\n",
    "        \"\"\"Demonstrate Phase 2: Comparative Analysis\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"PHASE 2 DEMO: Per-Class Gating Comparative Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Base configuration\n",
    "        base_args = {\n",
    "            \"img_size\": self.config[\"img_size\"],\n",
    "            \"backbone\": \"resnet18\",\n",
    "            \"num_classes\": self.config[\"num_classes\"],\n",
    "            \"gate_temp\": 100,\n",
    "            \"loss_keys\": [\"b1\", \"b2\", \"b3\", \"gate\", \"divAttn\", \"total\"],\n",
    "            \"acc_keys\": [\"acc1\", \"acc2\", \"acc3\", \"accGate\"],\n",
    "            \"loss_wgts\": [0.7, 1.0, 0.01],\n",
    "            \"enhanced_diversity\": True,\n",
    "            \"diversity_type\": \"cosine\",\n",
    "        }\n",
    "\n",
    "        # Configurations to compare\n",
    "        configurations = {\n",
    "            \"global_gating\": {\n",
    "                \"name\": \"Global Gating\",\n",
    "                \"use_per_class_gating\": False,\n",
    "                \"use_label_correlation\": False,\n",
    "            },\n",
    "            \"per_class_gating\": {\n",
    "                \"name\": \"Per-Class Gating\",\n",
    "                \"use_per_class_gating\": True,\n",
    "                \"use_label_correlation\": False,\n",
    "            },\n",
    "            \"enhanced_per_class\": {\n",
    "                \"name\": \"Enhanced Per-Class\",\n",
    "                \"use_per_class_gating\": True,\n",
    "                \"use_label_correlation\": True,\n",
    "                \"gating_regularization\": 0.01,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Create comparative framework\n",
    "        framework = ComparativeTrainingFramework(base_args)\n",
    "        criterion = {\"bce\": nn.BCEWithLogitsLoss()}\n",
    "\n",
    "        phase2_results = {}\n",
    "\n",
    "        for config_key, config_opts in configurations.items():\n",
    "            print(f\"\\n--- Training {config_opts['name']} ---\")\n",
    "\n",
    "            # Merge configuration\n",
    "            args = base_args.copy()\n",
    "            args.update({k: v for k, v in config_opts.items() if k != \"name\"})\n",
    "\n",
    "            # Create model\n",
    "            model = MultiLabelMEDAFv2(args)\n",
    "            model.to(self.device)\n",
    "\n",
    "            # Print model info\n",
    "            summary = model.get_gating_summary()\n",
    "            param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "            print(f\"Model: {summary['gating_type']} gating\")\n",
    "            print(f\"Parameters: {param_count:,}\")\n",
    "            print(f\"Label correlation: {summary['use_label_correlation']}\")\n",
    "\n",
    "            # Training\n",
    "            optimizer = torch.optim.Adam(\n",
    "                model.parameters(), lr=self.config[\"learning_rate\"]\n",
    "            )\n",
    "\n",
    "            metrics_history = []\n",
    "            best_acc = 0\n",
    "\n",
    "            for epoch in range(self.config[\"num_epochs\"]):\n",
    "                framework.current_epoch = epoch\n",
    "\n",
    "                metrics = train_multilabel_v2(\n",
    "                    self.train_loader,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    args,\n",
    "                    self.device,\n",
    "                    framework,\n",
    "                )\n",
    "\n",
    "                metrics_history.append(metrics)\n",
    "\n",
    "                if metrics[\"subset_acc\"] > best_acc:\n",
    "                    best_acc = metrics[\"subset_acc\"]\n",
    "\n",
    "                if epoch % 2 == 0:\n",
    "                    print(\n",
    "                        f\"Epoch {epoch}: Loss={metrics['total_loss']:.4f}, Acc={metrics['subset_acc']:.2f}%\"\n",
    "                    )\n",
    "\n",
    "            phase2_results[config_key] = {\n",
    "                \"model\": model,\n",
    "                \"config\": config_opts,\n",
    "                \"best_accuracy\": best_acc,\n",
    "                \"final_metrics\": metrics_history[-1],\n",
    "                \"metrics_history\": metrics_history,\n",
    "            }\n",
    "\n",
    "        # Print comparative analysis\n",
    "        framework.print_comparison()\n",
    "\n",
    "        self.results[\"phase2\"] = phase2_results\n",
    "\n",
    "    def analyze_attention_patterns(self):\n",
    "        \"\"\"Analyze attention patterns between global and per-class gating\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ATTENTION PATTERN ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        if \"phase2\" not in self.results:\n",
    "            print(\"Phase 2 results not available for analysis\")\n",
    "            return\n",
    "\n",
    "        # Get sample batch\n",
    "        sample_batch = next(iter(self.test_loader))\n",
    "        inputs, targets = sample_batch[0][:4].to(self.device), sample_batch[1][:4].to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        print(f\"Analyzing batch with shape: {inputs.shape}\")\n",
    "        print(f\"Target labels:\\n{targets}\")\n",
    "\n",
    "        # Analyze global vs per-class gating\n",
    "        global_model = self.results[\"phase2\"][\"global_gating\"][\"model\"]\n",
    "        per_class_model = self.results[\"phase2\"][\"per_class_gating\"][\"model\"]\n",
    "\n",
    "        global_model.eval()\n",
    "        per_class_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Global gating analysis\n",
    "            global_outputs = global_model(\n",
    "                inputs, targets, return_attention_weights=True\n",
    "            )\n",
    "            global_gate_pred = global_outputs[\"gate_pred\"]\n",
    "\n",
    "            print(f\"\\nGlobal Gating Weights (averaged across samples):\")\n",
    "            print(f\"Expert preferences: {global_gate_pred.mean(dim=0)}\")\n",
    "\n",
    "            # Per-class gating analysis\n",
    "            pc_outputs = per_class_model(inputs, targets, return_attention_weights=True)\n",
    "            if \"per_class_weights\" in pc_outputs:\n",
    "                pc_weights = pc_outputs[\"per_class_weights\"]\n",
    "\n",
    "                print(f\"\\nPer-Class Gating Weights:\")\n",
    "                print(f\"Shape: {pc_weights.shape}\")\n",
    "\n",
    "                # Average expert preferences per class\n",
    "                avg_class_prefs = pc_weights.mean(dim=0)\n",
    "                print(f\"Average expert preferences per class:\")\n",
    "                for class_idx in range(avg_class_prefs.shape[0]):\n",
    "                    expert_prefs = avg_class_prefs[class_idx]\n",
    "                    dominant_expert = expert_prefs.argmax().item()\n",
    "                    max_pref = expert_prefs.max().item()\n",
    "                    print(\n",
    "                        f\"  Class {class_idx}: Expert {dominant_expert} ({max_pref:.3f}) - {expert_prefs}\"\n",
    "                    )\n",
    "\n",
    "                # Measure specialization\n",
    "                expert_entropy = -(\n",
    "                    avg_class_prefs * torch.log(avg_class_prefs + 1e-8)\n",
    "                ).sum(dim=-1)\n",
    "                avg_entropy = expert_entropy.mean().item()\n",
    "\n",
    "                print(f\"\\nSpecialization Analysis:\")\n",
    "                print(\n",
    "                    f\"Average gating entropy: {avg_entropy:.3f} (lower = more specialized)\"\n",
    "                )\n",
    "                print(f\"Class entropies: {expert_entropy}\")\n",
    "\n",
    "                # Expert usage distribution\n",
    "                expert_usage = avg_class_prefs.mean(dim=0)\n",
    "                print(f\"Overall expert usage: {expert_usage}\")\n",
    "\n",
    "    def plot_training_curves(self):\n",
    "        \"\"\"Plot training curves for comparison\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"PLOTTING TRAINING CURVES\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        if \"phase2\" not in self.results:\n",
    "            print(\"Phase 2 results not available for plotting\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "        except ImportError as exc:\n",
    "            print(f\"Matplotlib not available: {exc}\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Loss curves\n",
    "        plt.subplot(1, 3, 1)\n",
    "        for config_key, results in self.results[\"phase2\"].items():\n",
    "            metrics_history = results[\"metrics_history\"]\n",
    "            losses = [m[\"total_loss\"] for m in metrics_history]\n",
    "            plt.plot(losses, label=results[\"config\"][\"name\"])\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Accuracy curves\n",
    "        plt.subplot(1, 3, 2)\n",
    "        for config_key, results in self.results[\"phase2\"].items():\n",
    "            metrics_history = results[\"metrics_history\"]\n",
    "            accuracies = [m[\"subset_acc\"] for m in metrics_history]\n",
    "            plt.plot(accuracies, label=results[\"config\"][\"name\"])\n",
    "        plt.title(\"Subset Accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Diversity loss curves\n",
    "        plt.subplot(1, 3, 3)\n",
    "        for config_key, results in self.results[\"phase2\"].items():\n",
    "            metrics_history = results[\"metrics_history\"]\n",
    "            diversity_losses = [m[\"diversity_loss\"] for m in metrics_history]\n",
    "            plt.plot(diversity_losses, label=results[\"config\"][\"name\"])\n",
    "        plt.title(\"Diversity Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Diversity Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"multilabel_medaf_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Training curves saved as 'multilabel_medaf_comparison.png'\")\n",
    "\n",
    "    def print_final_summary(self):\n",
    "        \"\"\"Print comprehensive summary\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"MULTI-LABEL MEDAF DEMO SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        # Phase 1 summary\n",
    "        if \"phase1\" in self.results:\n",
    "            print(\"\\nðŸ“Š Phase 1 (Basic Multi-Label MEDAF):\")\n",
    "            print(f\"   Final Loss: {self.results['phase1']['final_loss']:.4f}\")\n",
    "            if \"checkpoint\" in self.results[\"phase1\"]:\n",
    "                print(f\"   Checkpoint: {self.results['phase1']['checkpoint']}\")\n",
    "\n",
    "        # Phase 2 summary\n",
    "        if \"phase2\" in self.results:\n",
    "            print(\"\\nðŸŽ¯ Phase 2 (Per-Class Gating Comparative Analysis):\")\n",
    "\n",
    "            for config_key, results in self.results[\"phase2\"].items():\n",
    "                config_name = results[\"config\"][\"name\"]\n",
    "                best_acc = results[\"best_accuracy\"]\n",
    "                final_loss = results[\"final_metrics\"][\"total_loss\"]\n",
    "\n",
    "                print(\n",
    "                    f\"   {config_name:20}: Acc={best_acc:6.2f}%, Loss={final_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "            # Find best configuration\n",
    "            best_config = max(\n",
    "                self.results[\"phase2\"].items(), key=lambda x: x[1][\"best_accuracy\"]\n",
    "            )\n",
    "\n",
    "            print(f\"\\nðŸ† Best Configuration: {best_config[1]['config']['name']}\")\n",
    "            print(f\"   Best Accuracy: {best_config[1]['best_accuracy']:.2f}%\")\n",
    "\n",
    "            # Calculate improvements\n",
    "            if (\n",
    "                \"global_gating\" in self.results[\"phase2\"]\n",
    "                and \"per_class_gating\" in self.results[\"phase2\"]\n",
    "            ):\n",
    "                global_acc = self.results[\"phase2\"][\"global_gating\"][\"best_accuracy\"]\n",
    "                pc_acc = self.results[\"phase2\"][\"per_class_gating\"][\"best_accuracy\"]\n",
    "                improvement = pc_acc - global_acc\n",
    "\n",
    "                print(f\"\\nðŸ“ˆ Per-Class Gating Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "                if improvement > 0:\n",
    "                    print(\"   âœ… Per-class gating shows performance benefits!\")\n",
    "                else:\n",
    "                    print(\n",
    "                        \"   â„¹ï¸  Results may vary with longer training and real datasets\"\n",
    "                    )\n",
    "\n",
    "        train_count = len(self.train_loader.dataset) if self.train_loader else 0\n",
    "        val_count = len(self.val_loader.dataset) if self.val_loader else 0\n",
    "\n",
    "        print(f\"\\nðŸ”§ Configuration Used:\")\n",
    "        print(\n",
    "            f\"   Dataset: {self.dataset_name} | train {train_count}, val {val_count}, {self.config['num_classes']} classes\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   Training: {self.config['num_epochs']} epochs, batch size {self.config['batch_size']}\"\n",
    "        )\n",
    "        print(f\"   Device: {self.device}\")\n",
    "\n",
    "        print(f\"\\nðŸ“ Key Insights:\")\n",
    "        print(f\"   â€¢ Multi-label MEDAF successfully handles multiple labels per sample\")\n",
    "        print(f\"   â€¢ Per-class gating enables class-specific expert specialization\")\n",
    "        print(\n",
    "            f\"   â€¢ Attention diversity encourages experts to focus on different regions\"\n",
    "        )\n",
    "        print(f\"   â€¢ Configurable architecture allows easy experimentation\")\n",
    "\n",
    "        print(\"\\nðŸš€ Next Steps:\")\n",
    "        print(\"   1. Experiment with real multi-label datasets (PASCAL VOC, MS-COCO)\")\n",
    "        print(\"   2. Conduct comprehensive ablation studies\")\n",
    "        print(\"   3. Implement advanced research extensions\")\n",
    "        print(\"   4. Scale to larger models and datasets\")\n",
    "\n",
    "    def run_demo(self):\n",
    "        \"\"\"Run complete demonstration\"\"\"\n",
    "        print(\"ðŸŽ¬ Multi-Label MEDAF Complete Demonstration\")\n",
    "        if self.config.get(\"run_phase2\"):\n",
    "            print(\"Phase 1: Basic Multi-Label + Phase 2: Per-Class Gating\")\n",
    "        else:\n",
    "            print(\"Phase 1: Basic Multi-Label Training\")\n",
    "\n",
    "        # Create dataset\n",
    "        self.create_dataset()\n",
    "\n",
    "        # Demo Phase 1\n",
    "        self.demo_phase1()\n",
    "\n",
    "        # Demo Phase 2 with comparative analysis\n",
    "        if self.config.get(\"run_phase2\"):\n",
    "            self.demo_phase2_comparative()\n",
    "\n",
    "        # Analyze attention patterns\n",
    "        if self.config.get(\"run_phase2\"):\n",
    "            self.analyze_attention_patterns()\n",
    "\n",
    "        # Plot results\n",
    "        if self.config.get(\"run_phase2\"):\n",
    "            try:\n",
    "                self.plot_training_curves()\n",
    "            except Exception as e:\n",
    "                print(f\"Plotting failed: {e} (matplotlib may not be available)\")\n",
    "\n",
    "        # Final summary\n",
    "        self.print_final_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe24bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Load Phase 1 Checkpoint\n",
    "def load_phase1_checkpoint(\n",
    "    checkpoint_path: Union[str, Path],\n",
    "    device: Union[str, torch.device, None] = None,\n",
    "):\n",
    "    \"\"\"Load a saved Phase 1 MEDAF checkpoint.\"\"\"\n",
    "\n",
    "    device_obj = torch.device(device) if device else torch.device(\"cpu\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device_obj)\n",
    "\n",
    "    args = checkpoint.get(\"args\")\n",
    "    if args is None:\n",
    "        raise KeyError(\"Checkpoint is missing 'args'.\")\n",
    "\n",
    "    model = MultiLabelMEDAF(args)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    model.to(device_obj)\n",
    "    model.eval()\n",
    "\n",
    "    return model, checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd712bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override: MultiLabelMEDAF (from core.multilabel_net.py)\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from core.net import build_backbone, conv1x1, Classifier\n",
    "\n",
    "class MultiLabelMEDAF(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Label version of MEDAF (Multi-Expert Diverse Attention Fusion)\n",
    "\n",
    "    Key changes from original MEDAF:\n",
    "    1. Support for multi-hot label targets\n",
    "    2. BCEWithLogitsLoss instead of CrossEntropyLoss\n",
    "    3. Multi-label attention diversity computation\n",
    "    4. Per-sample CAM extraction for multiple positive classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args=None):\n",
    "        super(MultiLabelMEDAF, self).__init__()\n",
    "        backbone, feature_dim, self.cam_size = build_backbone(\n",
    "            img_size=args[\"img_size\"],\n",
    "            backbone_name=args[\"backbone\"],\n",
    "            projection_dim=-1,\n",
    "            inchan=3,\n",
    "        )\n",
    "        self.img_size = args[\"img_size\"]\n",
    "        self.gate_temp = args[\"gate_temp\"]\n",
    "        self.num_classes = args[\"num_classes\"]\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Shared layers (L1-L3)\n",
    "        self.shared_l3 = nn.Sequential(*list(backbone.children())[:-6])\n",
    "\n",
    "        # Expert branch 1\n",
    "        self.branch1_l4 = nn.Sequential(*list(backbone.children())[-6:-3])\n",
    "        self.branch1_l5 = nn.Sequential(*list(backbone.children())[-3])\n",
    "        self.branch1_cls = conv1x1(feature_dim, self.num_classes)\n",
    "\n",
    "        # Expert branch 2 (deep copy)\n",
    "        self.branch2_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.branch2_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        self.branch2_cls = conv1x1(feature_dim, self.num_classes)\n",
    "\n",
    "        # Expert branch 3 (deep copy)\n",
    "        self.branch3_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.branch3_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        self.branch3_cls = conv1x1(feature_dim, self.num_classes)\n",
    "\n",
    "        # Gating network\n",
    "        self.gate_l3 = copy.deepcopy(self.shared_l3)\n",
    "        self.gate_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.gate_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        self.gate_cls = nn.Sequential(\n",
    "            Classifier(feature_dim, int(feature_dim / 4), bias=True),\n",
    "            Classifier(int(feature_dim / 4), 3, bias=True),  # 3 experts\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y=None, return_ft=False):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-label MEDAF\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor [B, C, H, W]\n",
    "            y: Multi-hot labels [B, num_classes] or None\n",
    "            return_ft: Whether to return features\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing logits, gate predictions, and CAMs/features\n",
    "        \"\"\"\n",
    "        b = x.size(0)\n",
    "        ft_till_l3 = self.shared_l3(x)\n",
    "\n",
    "        # Expert branch 1\n",
    "        branch1_l4 = self.branch1_l4(ft_till_l3.clone())\n",
    "        branch1_l5 = self.branch1_l5(branch1_l4)\n",
    "        b1_ft_cams = self.branch1_cls(branch1_l5)  # [B, num_classes, H, W]\n",
    "        b1_logits = self.avg_pool(b1_ft_cams).view(b, -1)\n",
    "\n",
    "        # Expert branch 2\n",
    "        branch2_l4 = self.branch2_l4(ft_till_l3.clone())\n",
    "        branch2_l5 = self.branch2_l5(branch2_l4)\n",
    "        b2_ft_cams = self.branch2_cls(branch2_l5)  # [B, num_classes, H, W]\n",
    "        b2_logits = self.avg_pool(b2_ft_cams).view(b, -1)\n",
    "\n",
    "        # Expert branch 3\n",
    "        branch3_l4 = self.branch3_l4(ft_till_l3.clone())\n",
    "        branch3_l5 = self.branch3_l5(branch3_l4)\n",
    "        b3_ft_cams = self.branch3_cls(branch3_l5)  # [B, num_classes, H, W]\n",
    "        b3_logits = self.avg_pool(b3_ft_cams).view(b, -1)\n",
    "\n",
    "        # Store CAMs for diversity loss computation\n",
    "        cams_list = [b1_ft_cams, b2_ft_cams, b3_ft_cams]\n",
    "\n",
    "        # Multi-label CAM extraction for positive classes\n",
    "        if y is not None:\n",
    "            multi_label_cams = self._extract_multilabel_cams(cams_list, y)\n",
    "        else:\n",
    "            multi_label_cams = None\n",
    "\n",
    "        if return_ft:\n",
    "            # Aggregate features from all experts\n",
    "            fts = (\n",
    "                b1_ft_cams.detach().clone()\n",
    "                + b2_ft_cams.detach().clone()\n",
    "                + b3_ft_cams.detach().clone()\n",
    "            )\n",
    "\n",
    "        # Gating network\n",
    "        gate_l5 = self.gate_l5(self.gate_l4(self.gate_l3(x)))\n",
    "        gate_pool = self.avg_pool(gate_l5).view(b, -1)\n",
    "        gate_pred = F.softmax(self.gate_cls(gate_pool) / self.gate_temp, dim=1)\n",
    "\n",
    "        # Adaptive fusion using gating weights\n",
    "        gate_logits = torch.stack(\n",
    "            [b1_logits.detach(), b2_logits.detach(), b3_logits.detach()], dim=-1\n",
    "        )\n",
    "        gate_logits = gate_logits * gate_pred.view(\n",
    "            gate_pred.size(0), 1, gate_pred.size(1)\n",
    "        )\n",
    "        gate_logits = gate_logits.sum(-1)\n",
    "\n",
    "        logits_list = [b1_logits, b2_logits, b3_logits, gate_logits]\n",
    "\n",
    "        if return_ft and y is None:\n",
    "            outputs = {\n",
    "                \"logits\": logits_list,\n",
    "                \"gate_pred\": gate_pred,\n",
    "                \"fts\": fts,\n",
    "                \"cams_list\": cams_list,\n",
    "            }\n",
    "        else:\n",
    "            outputs = {\n",
    "                \"logits\": logits_list,\n",
    "                \"gate_pred\": gate_pred,\n",
    "                \"multi_label_cams\": multi_label_cams,\n",
    "                \"cams_list\": cams_list,\n",
    "            }\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _extract_multilabel_cams(self, cams_list, targets):\n",
    "        \"\"\"\n",
    "        Extract CAMs for all positive classes in multi-label setting\n",
    "        \"\"\"\n",
    "        batch_size = targets.size(0)\n",
    "        extracted_cams = []\n",
    "\n",
    "        for expert_idx, expert_cams in enumerate(cams_list):\n",
    "            expert_extracted = []\n",
    "\n",
    "            for batch_idx in range(batch_size):\n",
    "                # Find positive class indices for this sample\n",
    "                positive_classes = torch.where(targets[batch_idx] == 1)[0]\n",
    "\n",
    "                if len(positive_classes) > 0:\n",
    "                    # Extract CAMs for positive classes\n",
    "                    sample_cams = expert_cams[\n",
    "                        batch_idx, positive_classes\n",
    "                    ]  # [num_positive, H, W]\n",
    "                    expert_extracted.append(sample_cams)\n",
    "                else:\n",
    "                    # If no positive classes, create zero tensor\n",
    "                    H, W = expert_cams.shape[-2:]\n",
    "                    expert_extracted.append(\n",
    "                        torch.zeros(1, H, W, device=expert_cams.device)\n",
    "                    )\n",
    "\n",
    "            extracted_cams.append(expert_extracted)\n",
    "\n",
    "        return extracted_cams\n",
    "\n",
    "    def get_params(self, prefix=\"extractor\"):\n",
    "        \"\"\"Get model parameters for different learning rates\"\"\"\n",
    "        extractor_params = (\n",
    "            list(self.shared_l3.parameters())\n",
    "            + list(self.branch1_l4.parameters())\n",
    "            + list(self.branch1_l5.parameters())\n",
    "            + list(self.branch2_l4.parameters())\n",
    "            + list(self.branch2_l5.parameters())\n",
    "            + list(self.branch3_l4.parameters())\n",
    "            + list(self.branch3_l5.parameters())\n",
    "            + list(self.gate_l3.parameters())\n",
    "            + list(self.gate_l4.parameters())\n",
    "            + list(self.gate_l5.parameters())\n",
    "        )\n",
    "        extractor_params_ids = list(map(id, extractor_params))\n",
    "        classifier_params = filter(\n",
    "            lambda p: id(p) not in extractor_params_ids, self.parameters()\n",
    "        )\n",
    "\n",
    "        if prefix in [\"extractor\", \"extract\"]:\n",
    "            return extractor_params\n",
    "        elif prefix in [\"classifier\"]:\n",
    "            return classifier_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a4477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override: Training utilities (from core.multilabel_train.py)\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from misc.util import *\n",
    "\n",
    "\n",
    "def multiLabelAttnDiv(cams_list, targets, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Multi-label attention diversity loss\n",
    "    \"\"\"\n",
    "    if targets is None or targets.sum() == 0:\n",
    "        return torch.tensor(0.0, device=cams_list[0].device)\n",
    "\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=eps)\n",
    "    diversity_loss = 0.0\n",
    "    total_pairs = 0\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        positive_classes = torch.where(targets[batch_idx] == 1)[0]\n",
    "        if len(positive_classes) == 0:\n",
    "            continue\n",
    "\n",
    "        for class_idx in positive_classes:\n",
    "            expert_cams = torch.stack(\n",
    "                [\n",
    "                    cams_list[0][batch_idx, class_idx],\n",
    "                    cams_list[1][batch_idx, class_idx],\n",
    "                    cams_list[2][batch_idx, class_idx],\n",
    "                ]\n",
    "            )\n",
    "            expert_cams = expert_cams.view(3, -1)\n",
    "            expert_cams = F.normalize(expert_cams, p=2, dim=-1)\n",
    "            mean = expert_cams.mean(dim=-1, keepdim=True)\n",
    "            expert_cams = F.relu(expert_cams - mean)\n",
    "\n",
    "            for i in range(3):\n",
    "                for j in range(i + 1, 3):\n",
    "                    similarity = cos(\n",
    "                        expert_cams[i : i + 1], expert_cams[j : j + 1]\n",
    "                    ).mean()\n",
    "                    diversity_loss += similarity\n",
    "                    total_pairs += 1\n",
    "\n",
    "    if total_pairs > 0:\n",
    "        return diversity_loss / total_pairs\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=cams_list[0].device)\n",
    "\n",
    "\n",
    "def multiLabelAccuracy(predictions, targets, threshold=0.5):\n",
    "    \"\"\"Compute multi-label accuracy metrics\"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(predictions)\n",
    "        pred_binary = (probs > threshold).float()\n",
    "        subset_acc = (pred_binary == targets).all(dim=1).float().mean()\n",
    "        hamming_acc = (pred_binary == targets).float().mean()\n",
    "        tp = (pred_binary * targets).sum(dim=0)\n",
    "        fp = (pred_binary * (1 - targets)).sum(dim=0)\n",
    "        fn = ((1 - pred_binary) * targets).sum(dim=0)\n",
    "        precision = tp / (tp + fp + 1e-8)\n",
    "        recall = tp / (tp + fn + 1e-8)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "        precision = precision.mean()\n",
    "        recall = recall.mean()\n",
    "        f1 = f1.mean()\n",
    "    return subset_acc, hamming_acc, precision, recall, f1\n",
    "\n",
    "\n",
    "def train_multilabel(train_loader, model, criterion, optimizer, args, device=None):\n",
    "    \"\"\"Training loop for multi-label MEDAF\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    loss_keys = args[\"loss_keys\"]\n",
    "    acc_keys = args[\"acc_keys\"]\n",
    "\n",
    "    loss_meter = {p: AverageMeter() for p in loss_keys}\n",
    "    acc_meter = {p: AverageMeter() for p in acc_keys}\n",
    "    time_start = time.time()\n",
    "\n",
    "    for i, data_batch in enumerate(train_loader):\n",
    "        inputs = data_batch[0].to(device)\n",
    "        targets = data_batch[1].to(device)\n",
    "\n",
    "        output_dict = model(inputs, targets)\n",
    "        logits = output_dict[\"logits\"]\n",
    "        cams_list = output_dict[\"cams_list\"]\n",
    "\n",
    "        bce_losses = [\n",
    "            criterion[\"bce\"](logit.float(), targets.float()) for logit in logits[:3]\n",
    "        ]\n",
    "        gate_loss = criterion[\"bce\"](logits[3].float(), targets.float())\n",
    "        diversity_loss = multiLabelAttnDiv(cams_list, targets)\n",
    "\n",
    "        loss_values = bce_losses + [gate_loss, diversity_loss]\n",
    "        total_loss = (\n",
    "            args[\"loss_wgts\"][0] * sum(bce_losses)\n",
    "            + args[\"loss_wgts\"][1] * gate_loss\n",
    "            + args[\"loss_wgts\"][2] * diversity_loss\n",
    "        )\n",
    "        loss_values.append(total_loss)\n",
    "\n",
    "        acc_values = []\n",
    "        for logit in logits:\n",
    "            subset_acc, hamming_acc, _, _, _ = multiLabelAccuracy(logit, targets)\n",
    "            acc_values.append(subset_acc * 100)\n",
    "\n",
    "        multi_loss = {loss_keys[k]: loss_values[k] for k in range(len(loss_keys))}\n",
    "        train_accs = {acc_keys[k]: acc_values[k] for k in range(len(acc_keys))}\n",
    "\n",
    "        update_meter(loss_meter, multi_loss, inputs.size(0))\n",
    "        update_meter(acc_meter, train_accs, inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            tmp_str = f\"Batch [{i}/{len(train_loader)}] \"\n",
    "            tmp_str += \"< Training Loss >\\n\"\n",
    "            for k, v in loss_meter.items():\n",
    "                tmp_str += f\"{k}:{v.value:.4f} \"\n",
    "            tmp_str += \"\\n< Training Accuracy >\\n\"\n",
    "            for k, v in acc_meter.items():\n",
    "                tmp_str += f\"{k}:{v.value:.1f} \"\n",
    "            print(tmp_str)\n",
    "\n",
    "    time_elapsed = time.time() - time_start\n",
    "    print(f\"\\nEpoch completed in {time_elapsed:.1f}s\")\n",
    "\n",
    "    tmp_str = \"< Final Training Loss >\\n\"\n",
    "    for k, v in loss_meter.items():\n",
    "        tmp_str += f\"{k}:{v.value:.4f} \"\n",
    "    tmp_str += \"\\n< Final Training Accuracy >\\n\"\n",
    "    for k, v in acc_meter.items():\n",
    "        tmp_str += f\"{k}:{v.value:.1f} \"\n",
    "    print(tmp_str)\n",
    "\n",
    "    return loss_meter[loss_keys[-1]].value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override (v2): PerClassGating and LabelCorrelationModule\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from core.net import build_backbone, conv1x1, Classifier\n",
    "\n",
    "class PerClassGating(nn.Module):\n",
    "    \"\"\"Per-class gating mechanism for multi-label classification\"\"\"\n",
    "    def __init__(self, feature_dim, num_classes, num_experts=3, hidden_dim=None, dropout=0.1):\n",
    "        super(PerClassGating, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_experts = num_experts\n",
    "        self.feature_dim = feature_dim\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = feature_dim // 4\n",
    "        self.shared_transform = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim), nn.ReLU(), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.class_gates = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(hidden_dim // 2, num_experts),\n",
    "                )\n",
    "                for _ in range(num_classes)\n",
    "            ]\n",
    "        )\n",
    "        for gate in self.class_gates:\n",
    "            for layer in gate:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.normal_(layer.weight, 0, 0.01)\n",
    "                    nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, features, temperature=1.0):\n",
    "        batch_size = features.size(0)\n",
    "        shared_features = self.shared_transform(features)\n",
    "        gate_logits = []\n",
    "        for class_gate in self.class_gates:\n",
    "            logits = class_gate(shared_features)\n",
    "            gate_logits.append(logits)\n",
    "        gate_logits = torch.stack(gate_logits, dim=1)\n",
    "        gate_weights = F.softmax(gate_logits / temperature, dim=-1)\n",
    "        return gate_weights, gate_logits\n",
    "\n",
    "class LabelCorrelationModule(nn.Module):\n",
    "    \"\"\"Module to capture label co-occurrence patterns for better gating\"\"\"\n",
    "    def __init__(self, num_classes, embedding_dim=64):\n",
    "        super(LabelCorrelationModule, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.label_embeddings = nn.Embedding(num_classes, embedding_dim)\n",
    "        self.correlation_attention = nn.MultiheadAttention(embedding_dim, num_heads=4, batch_first=True)\n",
    "        self.output_proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, predicted_labels=None):\n",
    "        label_indices = torch.arange(self.num_classes, device=self.label_embeddings.weight.device)\n",
    "        all_embeddings = self.label_embeddings(label_indices)\n",
    "        batch_size = 1 if predicted_labels is None else predicted_labels.size(0)\n",
    "        label_emb = all_embeddings.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        corr_emb, _ = self.correlation_attention(label_emb, label_emb, label_emb)\n",
    "        correlation_features = self.output_proj(corr_emb)\n",
    "        return correlation_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49293c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override (v2): MultiLabelMEDAFv2\n",
    "class MultiLabelMEDAFv2(nn.Module):\n",
    "    \"\"\"Enhanced Multi-Label MEDAF with configurable per-class gating\"\"\"\n",
    "    def __init__(self, args=None):\n",
    "        super(MultiLabelMEDAFv2, self).__init__()\n",
    "        self.use_per_class_gating = args.get(\"use_per_class_gating\", False)\n",
    "        self.use_label_correlation = args.get(\"use_label_correlation\", False)\n",
    "        self.enhanced_diversity = args.get(\"enhanced_diversity\", False)\n",
    "        backbone, feature_dim, self.cam_size = build_backbone(\n",
    "            img_size=args[\"img_size\"],\n",
    "            backbone_name=args[\"backbone\"],\n",
    "            projection_dim=-1,\n",
    "            inchan=3,\n",
    "        )\n",
    "        self.img_size = args[\"img_size\"]\n",
    "        self.gate_temp = args[\"gate_temp\"]\n",
    "        self.num_classes = args[\"num_classes\"]\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.shared_l3 = nn.Sequential(*list(backbone.children())[:-6])\n",
    "        self.branch1_l4 = nn.Sequential(*list(backbone.children())[-6:-3])\n",
    "        self.branch1_l5 = nn.Sequential(*list(backbone.children())[-3])\n",
    "        self.branch1_cls = conv1x1(feature_dim, self.num_classes)\n",
    "        self.branch2_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.branch2_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        self.branch2_cls = conv1x1(feature_dim, self.num_classes)\n",
    "        self.branch3_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.branch3_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        self.branch3_cls = conv1x1(feature_dim, self.num_classes)\n",
    "        self.gate_l3 = copy.deepcopy(self.shared_l3)\n",
    "        self.gate_l4 = copy.deepcopy(self.branch1_l4)\n",
    "        self.gate_l5 = copy.deepcopy(self.branch1_l5)\n",
    "        if self.use_per_class_gating:\n",
    "            self.per_class_gating = PerClassGating(\n",
    "                feature_dim=feature_dim,\n",
    "                num_classes=self.num_classes,\n",
    "                num_experts=3,\n",
    "                dropout=args.get(\"gating_dropout\", 0.1),\n",
    "            )\n",
    "        else:\n",
    "            self.gate_cls = nn.Sequential(\n",
    "                Classifier(feature_dim, int(feature_dim / 4), bias=True),\n",
    "                Classifier(int(feature_dim / 4), 3, bias=True),\n",
    "            )\n",
    "        if self.use_label_correlation:\n",
    "            self.label_correlation = LabelCorrelationModule(\n",
    "                num_classes=self.num_classes,\n",
    "                embedding_dim=args.get(\"label_embedding_dim\", 64),\n",
    "            )\n",
    "\n",
    "    def forward(self, x, y=None, return_ft=False, return_attention_weights=False):\n",
    "        b = x.size(0)\n",
    "        ft_till_l3 = self.shared_l3(x)\n",
    "        branch1_l4 = self.branch1_l4(ft_till_l3.clone())\n",
    "        branch1_l5 = self.branch1_l5(branch1_l4)\n",
    "        b1_ft_cams = self.branch1_cls(branch1_l5)\n",
    "        b1_logits = self.avg_pool(b1_ft_cams).view(b, -1)\n",
    "        branch2_l4 = self.branch2_l4(ft_till_l3.clone())\n",
    "        branch2_l5 = self.branch2_l5(branch2_l4)\n",
    "        b2_ft_cams = self.branch2_cls(branch2_l5)\n",
    "        b2_logits = self.avg_pool(b2_ft_cams).view(b, -1)\n",
    "        branch3_l4 = self.branch3_l4(ft_till_l3.clone())\n",
    "        branch3_l5 = self.branch3_l5(branch3_l4)\n",
    "        b3_ft_cams = self.branch3_cls(branch3_l5)\n",
    "        b3_logits = self.avg_pool(b3_ft_cams).view(b, -1)\n",
    "        cams_list = [b1_ft_cams, b2_ft_cams, b3_ft_cams]\n",
    "        expert_logits = [b1_logits, b2_logits, b3_logits]\n",
    "        if y is not None:\n",
    "            multi_label_cams = self._extract_multilabel_cams(cams_list, y)\n",
    "        else:\n",
    "            multi_label_cams = None\n",
    "        if return_ft:\n",
    "            fts = b1_ft_cams.detach().clone() + b2_ft_cams.detach().clone() + b3_ft_cams.detach().clone()\n",
    "        gate_l5 = self.gate_l5(self.gate_l4(self.gate_l3(x)))\n",
    "        gate_features = self.avg_pool(gate_l5).view(b, -1)\n",
    "        if self.use_per_class_gating:\n",
    "            gate_weights, gate_logits = self.per_class_gating(gate_features, self.gate_temp)\n",
    "            expert_stack = torch.stack(expert_logits, dim=-1)\n",
    "            fused_logits = (expert_stack * gate_weights).sum(dim=-1)\n",
    "            gate_pred = gate_weights.mean(dim=1)\n",
    "        else:\n",
    "            gate_pred = F.softmax(self.gate_cls(gate_features) / self.gate_temp, dim=1)\n",
    "            gate_logits_stack = torch.stack(\n",
    "                [b1_logits.detach(), b2_logits.detach(), b3_logits.detach()], dim=-1\n",
    "            )\n",
    "            gate_logits_stack = gate_logits_stack * gate_pred.view(\n",
    "                gate_pred.size(0), 1, gate_pred.size(1)\n",
    "            )\n",
    "            fused_logits = gate_logits_stack.sum(-1)\n",
    "        logits_list = expert_logits + [fused_logits]\n",
    "        outputs = {\n",
    "            \"logits\": logits_list,\n",
    "            \"gate_pred\": gate_pred,\n",
    "            \"cams_list\": cams_list,\n",
    "            \"gating_type\": \"per_class\" if self.use_per_class_gating else \"global\",\n",
    "        }\n",
    "        if y is not None:\n",
    "            outputs[\"multi_label_cams\"] = multi_label_cams\n",
    "        if return_ft and y is None:\n",
    "            outputs[\"fts\"] = fts\n",
    "        if return_attention_weights and self.use_per_class_gating:\n",
    "            outputs[\"per_class_weights\"] = gate_weights\n",
    "            outputs[\"gate_logits\"] = gate_logits\n",
    "        if self.use_label_correlation:\n",
    "            corr_features = self.label_correlation()\n",
    "            outputs[\"correlation_features\"] = corr_features\n",
    "        return outputs\n",
    "\n",
    "    def _extract_multilabel_cams(self, cams_list, targets):\n",
    "        batch_size = targets.size(0)\n",
    "        extracted_cams = []\n",
    "        for expert_idx, expert_cams in enumerate(cams_list):\n",
    "            expert_extracted = []\n",
    "            for batch_idx in range(batch_size):\n",
    "                positive_classes = torch.where(targets[batch_idx] == 1)[0]\n",
    "                if len(positive_classes) > 0:\n",
    "                    sample_cams = expert_cams[batch_idx, positive_classes]\n",
    "                    expert_extracted.append(sample_cams)\n",
    "                else:\n",
    "                    H, W = expert_cams.shape[-2:]\n",
    "                    expert_extracted.append(torch.zeros(1, H, W, device=expert_cams.device))\n",
    "            extracted_cams.append(expert_extracted)\n",
    "        return extracted_cams\n",
    "\n",
    "    def get_params(self, prefix=\"extractor\"):\n",
    "        base_extractor_params = (\n",
    "            list(self.shared_l3.parameters())\n",
    "            + list(self.branch1_l4.parameters())\n",
    "            + list(self.branch1_l5.parameters())\n",
    "            + list(self.branch2_l4.parameters())\n",
    "            + list(self.branch2_l5.parameters())\n",
    "            + list(self.branch3_l4.parameters())\n",
    "            + list(self.branch3_l5.parameters())\n",
    "            + list(self.gate_l3.parameters())\n",
    "            + list(self.gate_l4.parameters())\n",
    "            + list(self.gate_l5.parameters())\n",
    "        )\n",
    "        if self.use_per_class_gating:\n",
    "            base_extractor_params.extend(list(self.per_class_gating.parameters()))\n",
    "        if self.use_label_correlation:\n",
    "            base_extractor_params.extend(list(self.label_correlation.parameters()))\n",
    "        extractor_params_ids = list(map(id, base_extractor_params))\n",
    "        classifier_params = filter(lambda p: id(p) not in extractor_params_ids, self.parameters())\n",
    "        if prefix in [\"extractor\", \"extract\"]:\n",
    "            return base_extractor_params\n",
    "        elif prefix in [\"classifier\"]:\n",
    "            return list(classifier_params)\n",
    "\n",
    "    def get_gating_summary(self):\n",
    "        summary = {\n",
    "            \"gating_type\": \"per_class\" if self.use_per_class_gating else \"global\",\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"use_label_correlation\": self.use_label_correlation,\n",
    "            \"enhanced_diversity\": self.enhanced_diversity,\n",
    "        }\n",
    "        if self.use_per_class_gating:\n",
    "            total_gate_params = sum(p.numel() for p in self.per_class_gating.parameters())\n",
    "            summary[\"gating_parameters\"] = total_gate_params\n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f3e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override (v2): Training and Evaluation Utilities\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from misc.util import *\n",
    "\n",
    "\n",
    "def enhancedMultiLabelAttnDiv(cams_list, targets, gate_weights=None, diversity_type=\"cosine\", eps=1e-6):\n",
    "    if targets is None or targets.sum() == 0:\n",
    "        return torch.tensor(0.0, device=cams_list[0].device)\n",
    "    diversity_loss = 0.0\n",
    "    total_pairs = 0\n",
    "    batch_size = targets.size(0)\n",
    "    for batch_idx in range(batch_size):\n",
    "        positive_classes = torch.where(targets[batch_idx] == 1)[0]\n",
    "        if len(positive_classes) == 0:\n",
    "            continue\n",
    "        for class_idx in positive_classes:\n",
    "            expert_cams = torch.stack(\n",
    "                [\n",
    "                    cams_list[0][batch_idx, class_idx],\n",
    "                    cams_list[1][batch_idx, class_idx],\n",
    "                    cams_list[2][batch_idx, class_idx],\n",
    "                ]\n",
    "            )\n",
    "            expert_cams = expert_cams.view(3, -1)\n",
    "            expert_cams = F.normalize(expert_cams, p=2, dim=-1)\n",
    "            mean = expert_cams.mean(dim=-1, keepdim=True)\n",
    "            expert_cams = F.relu(expert_cams - mean)\n",
    "            if gate_weights is not None:\n",
    "                class_gate_weights = gate_weights[batch_idx, class_idx]\n",
    "                expert_cams = expert_cams * class_gate_weights.unsqueeze(-1)\n",
    "            if diversity_type == \"cosine\":\n",
    "                cos = nn.CosineSimilarity(dim=1, eps=eps)\n",
    "                for i in range(3):\n",
    "                    for j in range(i + 1, 3):\n",
    "                        similarity = cos(expert_cams[i : i + 1], expert_cams[j : j + 1]).mean()\n",
    "                        diversity_loss += similarity\n",
    "                        total_pairs += 1\n",
    "            elif diversity_type == \"l2\":\n",
    "                for i in range(3):\n",
    "                    for j in range(i + 1, 3):\n",
    "                        l2_dist = torch.norm(expert_cams[i] - expert_cams[j], p=2)\n",
    "                        diversity_loss -= l2_dist\n",
    "                        total_pairs += 1\n",
    "            elif diversity_type == \"kl\":\n",
    "                expert_probs = F.softmax(expert_cams, dim=-1)\n",
    "                for i in range(3):\n",
    "                    for j in range(i + 1, 3):\n",
    "                        kl_div = F.kl_div(\n",
    "                            expert_probs[i : i + 1].log(), expert_probs[j : j + 1], reduction=\"batchmean\",\n",
    "                        )\n",
    "                        diversity_loss -= kl_div\n",
    "                        total_pairs += 1\n",
    "    return diversity_loss / max(total_pairs, 1)\n",
    "\n",
    "\n",
    "class ComparativeTrainingFramework:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.results = defaultdict(list)\n",
    "        self.current_epoch = 0\n",
    "    def log_metrics(self, model_type, epoch, metrics):\n",
    "        metrics_with_meta = {\"epoch\": epoch, \"model_type\": model_type, **metrics}\n",
    "        self.results[model_type].append(metrics_with_meta)\n",
    "    def get_comparison_summary(self):\n",
    "        summary = {}\n",
    "        for model_type, results in self.results.items():\n",
    "            if not results:\n",
    "                continue\n",
    "            latest = results[-1]\n",
    "            summary[model_type] = {\n",
    "                \"final_epoch\": latest[\"epoch\"],\n",
    "                \"final_subset_acc\": latest.get(\"subset_acc\", 0),\n",
    "                \"final_hamming_acc\": latest.get(\"hamming_acc\", 0),\n",
    "                \"final_f1\": latest.get(\"f1\", 0),\n",
    "                \"final_loss\": latest.get(\"total_loss\", 0),\n",
    "                \"avg_diversity_loss\": np.mean([r.get(\"diversity_loss\", 0) for r in results[-10:]]),\n",
    "            }\n",
    "        return summary\n",
    "    def print_comparison(self):\n",
    "        summary = self.get_comparison_summary()\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"COMPARATIVE ANALYSIS: Global vs Per-Class Gating\")\n",
    "        print(\"=\" * 60)\n",
    "        if \"global\" in summary and \"per_class\" in summary:\n",
    "            global_results = summary[\"global\"]\n",
    "            pc_results = summary[\"per_class\"]\n",
    "            print(f\"{'Metric':<20} {'Global':<15} {'Per-Class':<15} {'Improvement':<15}\")\n",
    "            print(\"-\" * 65)\n",
    "            metrics = [\"final_subset_acc\", \"final_hamming_acc\", \"final_f1\"]\n",
    "            for metric in metrics:\n",
    "                global_val = global_results.get(metric, 0)\n",
    "                pc_val = pc_results.get(metric, 0)\n",
    "                improvement = ((pc_val - global_val) / max(global_val, 1e-8)) * 100\n",
    "                print(f\"{metric.replace('final_', ''):<20} {global_val:<15.4f} {pc_val:<15.4f} {improvement:+.2f}%\")\n",
    "            global_loss = global_results.get(\"final_loss\", float(\"inf\"))\n",
    "            pc_loss = pc_results.get(\"final_loss\", float(\"inf\"))\n",
    "            loss_improvement = ((global_loss - pc_loss) / max(global_loss, 1e-8)) * 100\n",
    "            print(f\"{'loss_reduction':<20} {global_loss:<15.4f} {pc_loss:<15.4f} {loss_improvement:+.2f}%\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def train_multilabel_v2(train_loader, model, criterion, optimizer, args, device=None, comparative_framework=None):\n",
    "    model.train()\n",
    "    loss_keys = args[\"loss_keys\"]\n",
    "    acc_keys = args[\"acc_keys\"]\n",
    "    loss_meter = {p: AverageMeter() for p in loss_keys}\n",
    "    acc_meter = {p: AverageMeter() for p in acc_keys}\n",
    "    diversity_meter = AverageMeter()\n",
    "    gating_entropy_meter = AverageMeter()\n",
    "    time_start = time.time()\n",
    "    gating_summary = model.get_gating_summary()\n",
    "    gating_type = gating_summary[\"gating_type\"]\n",
    "    print(f\"\\nTraining with {gating_type} gating...\")\n",
    "    for i, data_batch in enumerate(train_loader):\n",
    "        inputs = data_batch[0].to(device)\n",
    "        targets = data_batch[1].to(device)\n",
    "        output_dict = model(inputs, targets, return_attention_weights=True)\n",
    "        logits = output_dict[\"logits\"]\n",
    "        cams_list = output_dict[\"cams_list\"]\n",
    "        gate_pred = output_dict[\"gate_pred\"]\n",
    "        per_class_weights = output_dict.get(\"per_class_weights\", None)\n",
    "        bce_losses = [criterion[\"bce\"](logit.float(), targets.float()) for logit in logits[:3]]\n",
    "        gate_loss = criterion[\"bce\"](logits[3].float(), targets.float())\n",
    "        if args.get(\"enhanced_diversity\", False) and per_class_weights is not None:\n",
    "            diversity_loss = enhancedMultiLabelAttnDiv(cams_list, targets, per_class_weights, diversity_type=args.get(\"diversity_type\", \"cosine\"))\n",
    "        else:\n",
    "            diversity_loss = multiLabelAttnDiv(cams_list, targets)\n",
    "        gating_reg_loss = 0.0\n",
    "        if per_class_weights is not None and args.get(\"gating_regularization\", 0) > 0:\n",
    "            gate_entropy = -(per_class_weights * torch.log(per_class_weights + 1e-8)).sum(dim=-1)\n",
    "            gating_reg_loss = -gate_entropy.mean()\n",
    "        loss_values = bce_losses + [gate_loss, diversity_loss]\n",
    "        total_loss = (\n",
    "            args[\"loss_wgts\"][0] * sum(bce_losses)\n",
    "            + args[\"loss_wgts\"][1] * gate_loss\n",
    "            + args[\"loss_wgts\"][2] * diversity_loss\n",
    "            + args.get(\"gating_regularization\", 0) * gating_reg_loss\n",
    "        )\n",
    "        loss_values.append(total_loss)\n",
    "        acc_values = []\n",
    "        for logit in logits:\n",
    "            subset_acc, hamming_acc, precision, recall, f1 = multiLabelAccuracy(logit, targets)\n",
    "            acc_values.append(subset_acc * 100)\n",
    "        multi_loss = {loss_keys[k]: loss_values[k] for k in range(len(loss_keys))}\n",
    "        train_accs = {acc_keys[k]: acc_values[k] for k in range(len(acc_keys))}\n",
    "        update_meter(loss_meter, multi_loss, inputs.size(0))\n",
    "        update_meter(acc_meter, train_accs, inputs.size(0))\n",
    "        diversity_meter.update(diversity_loss.item(), inputs.size(0))\n",
    "        if per_class_weights is not None:\n",
    "            gate_entropy = (-(per_class_weights * torch.log(per_class_weights + 1e-8)).sum(dim=-1).mean())\n",
    "            gating_entropy_meter.update(gate_entropy.item(), inputs.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 50 == 0:\n",
    "            tmp_str = f\"Batch [{i}/{len(train_loader)}] ({gating_type}) \"\n",
    "            tmp_str += f\"Loss: {total_loss.item():.4f}, \"\n",
    "            tmp_str += f\"Div: {diversity_loss.item():.6f}, \"\n",
    "            if per_class_weights is not None:\n",
    "                tmp_str += f\"GateEnt: {gating_entropy_meter.value:.3f}, \"\n",
    "            tmp_str += f\"SubsetAcc: {acc_values[3]:.2f}%\"\n",
    "            print(tmp_str)\n",
    "    time_elapsed = time.time() - time_start\n",
    "    final_metrics = {\n",
    "        \"total_loss\": loss_meter[loss_keys[-1]].value,\n",
    "        \"diversity_loss\": diversity_meter.value,\n",
    "        \"subset_acc\": acc_meter[acc_keys[-1]].value,\n",
    "        \"hamming_acc\": acc_meter[acc_keys[-1]].value,\n",
    "        \"gating_type\": gating_type,\n",
    "        \"training_time\": time_elapsed,\n",
    "    }\n",
    "    if per_class_weights is not None:\n",
    "        final_metrics[\"gating_entropy\"] = gating_entropy_meter.value\n",
    "    if comparative_framework is not None:\n",
    "        comparative_framework.log_metrics(gating_type, comparative_framework.current_epoch, final_metrics)\n",
    "    print(f\"\\nEpoch Summary ({gating_type} gating):\")\n",
    "    print(f\"  Total Loss: {final_metrics['total_loss']:.4f}\")\n",
    "    print(f\"  Diversity Loss: {final_metrics['diversity_loss']:.6f}\")\n",
    "    print(f\"  Subset Accuracy: {final_metrics['subset_acc']:.2f}%\")\n",
    "    if \"gating_entropy\" in final_metrics:\n",
    "        print(f\"  Gating Entropy: {final_metrics['gating_entropy']:.3f}\")\n",
    "    print(f\"  Training Time: {time_elapsed:.1f}s\")\n",
    "    return final_metrics\n",
    "\n",
    "\n",
    "def evaluate_multilabel_v2(model, test_loader, criterion, args, device=None):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_gate_weights = []\n",
    "    eval_loss = 0.0\n",
    "    eval_diversity = 0.0\n",
    "    num_batches = 0\n",
    "    gating_type = model.get_gating_summary()[\"gating_type\"]\n",
    "    with torch.no_grad():\n",
    "        for data_batch in test_loader:\n",
    "            inputs, targets = data_batch[0].to(device), data_batch[1].to(device)\n",
    "            outputs = model(inputs, targets, return_attention_weights=True)\n",
    "            logits = outputs[\"logits\"]\n",
    "            cams_list = outputs[\"cams_list\"]\n",
    "            fused_logits = logits[3]\n",
    "            bce_loss = criterion[\"bce\"](fused_logits, targets)\n",
    "            diversity_loss = multiLabelAttnDiv(cams_list, targets)\n",
    "            eval_loss += bce_loss.item()\n",
    "            eval_diversity += diversity_loss.item()\n",
    "            num_batches += 1\n",
    "            all_predictions.append(torch.sigmoid(fused_logits))\n",
    "            all_targets.append(targets)\n",
    "            if \"per_class_weights\" in outputs:\n",
    "                all_gate_weights.append(outputs[\"per_class_weights\"])\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    subset_acc, hamming_acc, precision, recall, f1 = multiLabelAccuracy(all_predictions, all_targets, threshold=0.5)\n",
    "    eval_metrics = {\n",
    "        \"eval_loss\": eval_loss / num_batches,\n",
    "        \"eval_diversity\": eval_diversity / num_batches,\n",
    "        \"subset_accuracy\": subset_acc.item(),\n",
    "        \"hamming_accuracy\": hamming_acc.item(),\n",
    "        \"precision\": precision.item(),\n",
    "        \"recall\": recall.item(),\n",
    "        \"f1_score\": f1.item(),\n",
    "        \"gating_type\": gating_type,\n",
    "    }\n",
    "    if all_gate_weights:\n",
    "        all_gate_weights = torch.cat(all_gate_weights, dim=0)\n",
    "        expert_preferences = all_gate_weights.mean(dim=0)\n",
    "        expert_entropy = -(expert_preferences * torch.log(expert_preferences + 1e-8)).sum(dim=-1)\n",
    "        eval_metrics.update({\n",
    "            \"avg_gating_entropy\": expert_entropy.mean().item(),\n",
    "            \"expert_specialization\": expert_preferences.std(dim=0).mean().item(),\n",
    "            \"max_expert_preference\": expert_preferences.max().item(),\n",
    "            \"min_expert_preference\": expert_preferences.min().item(),\n",
    "        })\n",
    "    return eval_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a53425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config for running the demo from notebook\n",
    "config = {\n",
    "    \"data_source\": \"chestxray\",  # or \"synthetic\"\n",
    "    \"known_csv\": str(DEFAULT_KNOWN_CSV),\n",
    "    \"image_root\": str(DEFAULT_IMAGE_ROOT),\n",
    "    \"batch_size\": 16,\n",
    "    \"num_epochs\": 2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"val_ratio\": 0.1,\n",
    "    \"num_workers\": 2,\n",
    "    # \"max_samples\": None,\n",
    "    \"max_samples\": 200,  # for quick iterations in notebook\n",
    "    \"phase1_checkpoint\": \"medaf_phase1_chestxray.pt\",\n",
    "    \"checkpoint_dir\": str(DEFAULT_CHECKPOINT_DIR),\n",
    "    \"run_phase2\": True,\n",
    "}\n",
    "\n",
    "print(\"Notebook demo config set.\")\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the demo from notebook\n",
    "# Set seeds for reproducibility\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize and run\n",
    "_demo = MultiLabelMEDAFDemo(config)\n",
    "_demo.run_demo()\n",
    "\n",
    "# Optionally evaluate v2 models if run_phase2\n",
    "if config.get(\"run_phase2\") and hasattr(_demo, \"results\") and \"phase2\" in _demo.results:\n",
    "    try:\n",
    "        criterion = {\"bce\": nn.BCEWithLogitsLoss()}\n",
    "        for key, res in _demo.results[\"phase2\"].items():\n",
    "            print(f\"\\nEvaluating {res['config']['name']} model...\")\n",
    "            eval_metrics = evaluate_multilabel_v2(res[\"model\"], _demo.test_loader, criterion, {}, device=_demo.device)\n",
    "            print(eval_metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation skipped due to error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
